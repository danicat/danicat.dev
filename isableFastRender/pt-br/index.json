


[{"content":"Olá e seja bem-vindo(a)! Sou Daniela Petruzalek e este é meu blog pessoal. Sou uma tecnóloga com experiência em engenharia de back-end e dados, e atualmente sou Developer Relations Engineer no Google. Neste blog falo sobre tecnologia, boas práticas, carreira e às vezes gatos. Você pode saber mais sobre meu perfil na página Sobre.\nAviso: as opiniões escritas neste blog são minhas e não representam necessariamente as opiniões do meu empregador.\n","date":"7 julho 2025","externalUrl":null,"permalink":"/pt-br/","section":"danicat.dev","summary":"Um blog para os amantes de tecnologia, gatos e café =^.^=","title":"danicat.dev","type":"page"},{"content":"","date":"7 julho 2025","externalUrl":null,"permalink":"/pt-br/events/","section":"Events","summary":"","title":"Events","type":"events"},{"content":"","date":"7 julho 2025","externalUrl":null,"permalink":"/pt-br/events/20250806-gophercon-south-africa/","section":"Events","summary":"","title":"Gophercon South Africa","type":"events"},{"content":" Descrição # GopherCon UK é um evento anual com dois dias de conferência multi-track e um dia de workshop, realizado na Brewery, no coração de Londres. Três dias de palestras incríveis, muitas oportunidades de networking e ótimos eventos sociais. A GopherCon UK oferece as informações e treinamentos mais atualizados sobre programação Go.\nDetalhes da Sessão # Detalhes específicos da sessão para 2025 ainda não estão disponíveis. Por favor, verifique o site do evento para atualizações.\n","date":"7 julho 2025","externalUrl":null,"permalink":"/pt-br/events/20250813-gophercon-uk/","section":"Events","summary":"","title":"Gophercon UK","type":"events"},{"content":"","date":"7 julho 2025","externalUrl":null,"permalink":"/pt-br/events/20250917-tdc-sao-paulo/","section":"Events","summary":"","title":"TDC Sao Paulo 2025","type":"events"},{"content":" Descrição # O principal evento mundial para desenvolvedores, inovadores em IA e líderes de tecnologia. Junte-se à maior reunião de inovadores de software, líderes de tecnologia e tomadores de decisão que estão moldando o futuro da tecnologia baseada em IA.\nDetalhes da Sessão # Detalhes específicos da sessão para 2025 ainda não estão disponíveis. Por favor, verifique o site do evento para atualizações.\n","date":"7 julho 2025","externalUrl":null,"permalink":"/pt-br/events/20250709-wearedevelopers-world-congress/","section":"Events","summary":"","title":"WeAreDevelopers World Congress 2025","type":"events"},{"content":"","date":"3 julho 2025","externalUrl":null,"permalink":"/pt-br/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":" Nota da autora: cerca de 90% deste post foi escrito por IA, mas eu revisei e editei para garantir que o post tivesse uma leitura agradável. Foi engraçado como o Jules tinha a tendência de se gabar de si mesmo. Tive que guiá-lo com muitos prompts para chegar a este resultado final, mas a última edição foi mais fácil de fazer manualmente. Você pode ver o histórico completo de edições no histórico de commits do PR. Uma menção notável é que ele se recusou completamente a traduzir este post para o português (Brasil) afirmando que não tem capacidade de tradução, mas a totalidade deste blog foi traduzida usando o Jules em uma interação anterior. Acho que não estava no clima. :)\nIntrodução # Recentemente, decidi atualizar a página inicial do meu blog para destacar melhor o conteúdo mais recente. Como engenheira de backend, mergulhar fundo nas complexidades do frontend não é o meu dia a dia, então, em vez de codificar manualmente todas as mudanças em um domínio menos familiar, pedi a ajuda do Jules, um assistente de codificação de IA.\nEste post detalha nossa jornada iterativa, os sucessos, os mal-entendidos (às vezes divertidos) e o que aprendi sobre como trabalhar efetivamente com IA para desenvolvimento web, especialmente para preencher lacunas de habilidades.\nO Objetivo: Uma Seção de Post em Destaque # Meu pedido inicial ao Jules foi direto:\n\u0026ldquo;Change the layout of the main page so that it displays the most recent blog post in highlight instead of it being in the recent posts list. The recent posts should contain all other posts except the most recent one. This behaviour should be seen only on the blog landing page (home). If the user clicks on the Blog menu it should still see all the posts in reverse chronological order, including the most recent one.\u0026rdquo;\nO Jules entendeu rapidamente e propôs um plano envolvendo explorar a codebase do Hugo, identificar templates e modificá-los.\nDestaques da Iteração: O Bom, O Mau e a IA # Nossa colaboração envolveu várias iterações para acertar tudo.\nIteração 1: Configuração Inicial - Acertando o Básico # O Jules identificou corretamente os partials do tema Blowfish e configurou a estrutura de sobreposição. A lógica para separar o post mais recente dos outros na lista de \u0026ldquo;Posts Recentes\u0026rdquo; foi bem implementada.\nO que funcionou: Entender a estrutura central do Hugo, buscar posts, modificações básicas de template. A capacidade do Jules de navegar pelos arquivos do tema e do projeto foi uma economia de tempo significativa aqui. Houve muita espera entre as tarefas Iteração 2: Estilizando com Tailwind - A Dança da Tentativa e Erro # Em seguida, focamos na aparência: título, largura e dimensões da imagem. Isso envolveu uma série de prompts para ajustar os visuais. Por exemplo:\n\u0026ldquo;Change the featured post title to \u0026lsquo;Featured Post\u0026rsquo;. Adjust its width to be about 80% of the view. The image is too tall/narrow, let\u0026rsquo;s try a 4:3 aspect ratio. That\u0026rsquo;s still not quite right, make it wider/less tall.\u0026rdquo;\nÉ aqui que a natureza iterativa de trabalhar com o Jules em elementos visuais se tornou muito aparente.\nAbordagem do Jules: Modificou arquivos i18n para títulos, usou várias classes de largura do Tailwind (por exemplo, md:w-4/5, md:w-2/3, max-w-xl, max-w-2xl) e manipulou o padding-bottom para as proporções da imagem.\nDesafio e Frustração: Um desafio particular, especialmente para alguém como eu que trabalha principalmente no backend, foi a natureza de tentativa e erro de estilizar com o Tailwind através de um intermediário. Embora o Jules pudesse aplicar as classes que achava apropriadas, o resultado visual nem sempre era imediatamente o que eu tinha em mente. As mudanças nas classes do Tailwind muitas vezes não se traduziam em uma diferença claramente visível na primeira tentativa, ou o efeito não era o esperado. Isso levou a algumas rodadas de \u0026ldquo;tente esta classe\u0026rdquo;, \u0026ldquo;não, faça mais estreito/largo/alto/baixo\u0026rdquo;, o que, embora bem-sucedido no final, podia ser frustrante às vezes. Isso destacou a desconexão entre o código e o feedback visual imediato neste fluxo de trabalho assíncrono assistido por IA.\nAprendizado: O ajuste fino da estética visual é a parte menos favorita da minha experiência, pois as instruções muitas vezes resultarão em falsos positivos. Um feedback claro e descritivo é fundamental, mas também reconhecer que alguma troca de informações é inevitável quando não posso apontar diretamente para uma tela ou fazer microajustes em tempo real. O Jules, no entanto, aplicou diligentemente cada mudança solicitada, o que ajudou a preencher minha lacuna de conhecimento em frontend.\nIteração 3: CSS Personalizado vs. Tailwind - Um Breve Desvio # Em um ponto, para obter um controle muito específico sobre as dimensões do card, eu pedi:\n\u0026ldquo;jules, instead of trying to use an existing style class, create an unique style class for the featured post card. This style should use relative width and height of 75% of the container\u0026hellip;\u0026rdquo;\nResposta do Jules: O Jules criou corretamente as regras de CSS personalizadas e refatorou o partial do card para usá-las.\nResultado e Aprendizado: Embora o Jules tenha implementado isso conforme solicitado, o resultado pareceu um tanto estranho ao resto do design do blog, que é fortemente baseado em Tailwind. O CSS personalizado não se harmonizou muito bem, e eu rapidamente decidi que manter a consistência com o Tailwind era mais importante. Esta foi uma boa lição para garantir que mesmo as soluções geradas por IA se encaixem na linguagem de design existente e na minha preferência por manter o framework estabelecido. O Jules voltou a se adaptar ao Tailwind mediante solicitação:\n\u0026ldquo;undo the last change and restore the tailwind style of formatting. apply the same style guidelines using tailwind best practices\u0026rdquo;\nIteração 4: O Grande Mal-Entendido dos \u0026ldquo;Comentários\u0026rdquo;! # Esta foi talvez a parte mais ilustrativa da interação humano-IA. Eu mencionei:\n\u0026ldquo;the comments are rendering in the featured post. please remove all the comments or make them invisible\u0026rdquo;\nInterpretação do Jules: O Jules presumiu que eu me referia ao sistema de comentários do usuário do blog (como Utterances ou Giscus) ou metadados como contagens de visualizações/curtidas. Isso levou a uma série de etapas em que o Jules tentou investigar e, em seguida, ocultar condicionalmente os metadados de visualizações/curtidas. Meu Esclarecimento: Após essas mudanças, eu esclareci dando um exemplo: \u0026ldquo;you are wrong, I never said I wanted to remove the views and likes - I\u0026rsquo;m referring to the code comments in rendering as {/* Adjusted padding \u0026hellip; /} and {/ Removed prose classes \u0026hellip; */}\u0026rdquo;\nResolução: Assim que o Jules entendeu que eu me referia a comentários literais de template Go/HTML que estavam formatados incorretamente (usando {/*...*} que não é um estilo de comentário válido do Hugo e, portanto, renderiza como texto) e não {{/* ... */}}, a correção foi imediata: remover o texto ofensivo dos templates. O que funcionou: A persistência e a abordagem sistemática do Jules para depurar o problema (mal-entendido) foram louváveis. Desafio e Aprendizado: Isso destacou um aspecto crucial da interação com a IA: a ambiguidade na linguagem natural. \u0026ldquo;Comentários\u0026rdquo; tem múltiplos significados. Meu relato inicial não foi preciso o suficiente. Iteração 5: Polimento Final # Depois de resolver os comentários visíveis do template, fizemos os ajustes finais:\n\u0026ldquo;Remove the \u0026lsquo;Featured Post\u0026rsquo; title. Change card width to 50%. Increase title and summary font sizes. Make the image\u0026rsquo;s aspect ratio 16:9.\u0026rdquo;\nIsso levou aos ajustes finais para a largura do card, tamanhos de fonte e proporção da imagem. A % da largura não teve nenhum efeito, mas mudar a proporção da imagem resolveu o problema.\nIteração Bônus: Jules Esboça Este Post do Blog # \u0026ldquo;This is perfect. No more code changes are needed. Now I want you to create a new blog post entry describing the iteration we just did\u0026hellip;\u0026rdquo;\nE aqui estamos nós! Este post foi esboçado com a ajuda do Jules, com base em nosso registro de interação e meu feedback orientador, incluindo os próprios pontos que você está lendo agora.\nO Que Funcionou Bem com o Jules # Preenchendo Lacunas de Habilidades: Como engenheira de backend, o Jules foi inestimável para lidar com tarefas de frontend envolvendo templates do Hugo e Tailwind CSS, áreas onde tenho menos experiência diária. O Jules compensou minha falta de conhecimento profundo em frontend, propondo e implementando soluções que eu poderia então guiar e refinar. Velocidade de Implementação: Para mudanças bem compreendidas, o Jules pode modificar código, criar arquivos e refatorar estruturas muito mais rápido do que a digitação manual. Lidando com Instruções Complexas: Geralmente, o Jules entendeu solicitações de várias etapas e objetivos de layout complexos. Resolução Sistemática de Problemas: Mesmo com mal-entendidos, o Jules muitas vezes seguia um processo lógico. Refinamento Iterativo: O Jules foi consistentemente receptivo a feedback para ajustes. Desafios e Aprendizados # Precisão da Linguagem: O incidente dos \u0026ldquo;comentários\u0026rdquo; ressalta o quão crítica é a linguagem precisa. O que é óbvio para um humano, ou uma abreviação, pode ser ambíguo para uma IA. Loop de Feedback Visual e Tailwind: A tentativa e erro com a estilização do Tailwind foi um desafio chave. Sem que o Jules \u0026ldquo;visse\u0026rdquo; o resultado, descrever os resultados visuais desejados ou por que um determinado conjunto de classes não estava funcionando como esperado exigia paciência e descrições detalhadas. Isso é inerente à interação baseada em texto para tarefas visuais. Má Interpretação e Correção de Rumo: Quando o Jules entendia mal uma tarefa, ele prosseguia diligentemente por esse caminho incorreto. Não havia como interrompê-lo no meio da tarefa; eu tinha que esperar que ele completasse sua sequência de ações atual antes de fornecer feedback corretivo. Fluxo de Trabalho Assíncrono e Ritmo: O trabalho é principalmente assíncrono. Cada solicitação e a implementação do Jules podiam levar de alguns minutos a, às vezes, meia hora para sequências mais complexas. Isso torna o ciclo iterativo mais lento do que a codificação direta com feedback instantâneo ou pair programming ao vivo. Recursos Sugeridos # Para os interessados em aprender mais sobre o Jules:\nJules Official Website Jules Documentation Conclusão # No geral, trabalhar com o Jules neste recurso da página inicial foi uma experiência produtiva. Realmente pareceu \u0026ldquo;vibe-coding\u0026rdquo; – uma troca dinâmica guiando a IA. A chave para o sucesso está na comunicação clara e iterativa, paciência durante os mal-entendidos e disposição para fornecer feedback específico e acionável.\nAs frustrações, particularmente com a tentativa e erro do Tailwind e as interpretações equivocadas ocasionais da IA, fazem parte do cenário atual do desenvolvimento assistido por IA. No entanto, apesar da natureza assíncrona e do tempo gasto para algumas gerações, a capacidade de descarregar os aspectos mecânicos da codificação e obter sugestões para áreas fora da minha especialidade principal (como implementações específicas do Tailwind ou estruturas do Hugo) ainda resultou em um ganho líquido. Foi significativamente mais rápido e eficaz do que se eu tivesse tentado aprender todos os princípios de design de frontend necessários, as complexidades do Hugo e as nuances do Tailwind CSS do zero para este recurso específico.\nAssistentes de IA como o Jules são ferramentas poderosas. Eles não substituem a supervisão humana ou a intenção do design, mas podem ser aceleradores incríveis quando abordados com a mentalidade e a estratégia de comunicação corretas.\n","date":"3 julho 2025","externalUrl":null,"permalink":"/pt-br/posts/20250703-jules-featured-post/","section":"Posts","summary":"Um relato detalhado do meu processo iterativo trabalhando com o Jules, um assistente de codificação de IA, para implementar uma nova seção de post em destaque na página inicial do meu blog.","title":"Como usei o Jules para adicionar um post em destaque a este blog","type":"posts"},{"content":"","date":"3 julho 2025","externalUrl":null,"permalink":"/pt-br/categories/interaction-logs/","section":"Categories","summary":"","title":"Interaction Logs","type":"categories"},{"content":"","date":"3 julho 2025","externalUrl":null,"permalink":"/pt-br/tags/jules/","section":"Tags","summary":"","title":"Jules","type":"tags"},{"content":"","date":"3 julho 2025","externalUrl":null,"permalink":"/pt-br/posts/","section":"Posts","summary":"","title":"Posts","type":"posts"},{"content":"","date":"3 julho 2025","externalUrl":null,"permalink":"/pt-br/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"},{"content":"","date":"3 julho 2025","externalUrl":null,"permalink":"/pt-br/tags/vibe-coding/","section":"Tags","summary":"","title":"Vibe-Coding","type":"tags"},{"content":"","date":"2 julho 2025","externalUrl":null,"permalink":"/pt-br/tags/cloud-assist/","section":"Tags","summary":"","title":"Cloud Assist","type":"tags"},{"content":" Introdução # Hoje vamos fazer um pequeno desvio do nosso conteúdo usual sobre agentes de IA para falar sobre um produto que explorei recentemente como parte da minha participação no I/O Connect Berlin 2025 na semana passada.\nEste evento reuniu mais de 1000 desenvolvedores de toda a Europa, incluindo membros das comunidades de desenvolvedores do Google (Google Developer Groups) e especialistas da comunidade. Foi também o meu primeiro evento oficial do Google desde que entrei para a equipe de DevRel em abril, por isso foi particularmente significativo para mim - e é por isso que não tivemos atualização no blog na semana passada!\nFui responsável por uma demonstração chamada “Design and Deploy”, que mostra a combinação de dois produtos: Application Design Center (ADC) e Gemini Cloud Assist (GCA). A demonstração foi tão bem recebida que achei que seria bom trazer esse conteúdo para o blog também, para dar a oportunidade às pessoas que não estavam lá de brincar com essa tecnologia também.\nO Application Design Center é um produto para ajudar arquitetos e desenvolvedores a projetar a infraestrutura de seus aplicativos. Na frente, ele fornece uma interface de usuário agradável onde você pode definir visualmente os componentes para sua infraestrutura, mas por baixo dos panos tudo na interface do usuário é representado como um módulo terraform para que você também possa aproveitar os benefícios da Infraestrutura como Código.\nUm aviso importante é que o ADC está atualmente em pré-visualização pública. Isso significa que o produto está evoluindo a cada dia e, às vezes, pode quebrar a compatibilidade com iterações anteriores. Ele também tem algumas arestas notavelmente ásperas que mencionarei abaixo, que devem ser resolvidas antes que o produto se torne disponível para o público em geral.\nO Gemini Cloud Assist (também em pré-visualização pública), por outro lado, é o nome oficial do produto para o suporte do Gemini no Google Cloud. Por causa disso, o GCA não é um produto autônomo, mas mais como um tecido conjuntivo que permite aos usuários interagir com qualquer coisa do GCP usando linguagem natural, incluindo todos os benefícios da moderna experiência de chatbot baseada em modelos de linguagem grandes.\nVamos ver como podemos usar ambas as tecnologias para projetar rapidamente a parte de infraestrutura de um aplicativo para nós.\nComo iniciar uma sessão de design de aplicativo # Você sempre pode abrir o Application Design Center manualmente no console do Google Cloud, mas qual é a graça disso? A melhor maneira de acionar o ADC para um novo design é simplesmente abrir o painel do Gemini em qualquer página. Aqui, por exemplo, estou usando a página de boas-vindas do meu projeto:\nTela de boas-vindas no console do Google Cloud Se você clicar no botão “estrela” no lado direito da barra de pesquisa, abrirá o painel do Gemini Cloud Assist:\nVisão ampliada do botão Gemini Deve abrir:\nTela de boas-vindas do Google Cloud Assist Este é o painel onde você pode interagir com o Gemini. Digite algo como “criar um aplicativo que faz x” e inclua quantos detalhes desejar sobre a arquitetura. Por exemplo, vamos tentar criar um aplicativo que gera fotos de gatos. Aqui está o prompt:\nCrie um aplicativo que gere fotos de gatos com o Gemini e as armazene em um banco de dados Cloud SQL. Os usuários podem solicitar novas fotos usando um serviço de geração e podem ver as fotos geradas com um serviço de fotos. Ambos os serviços são expostos por meio de um serviço de frontend e um balanceador de carga global.\nDepois de inserir o prompt, o Gemini pensará por um tempo e, após alguns segundos, produzirá uma saída como esta:\nResposta do Gemini com diagrama de arquitetura A visualização integrada nos dá uma ideia, mas podemos interagir melhor com o design se clicarmos no botão “Edit app design”. Isso abrirá o design em uma visualização expandida para que possamos refiná-lo ainda mais. (Observe que o restante deste artigo pressupõe que o botão “Edit app design” abre a janela de Pré-visualização. Se no seu caso não abrir, verifique as notas no final do artigo)\nÉ assim que fica na janela de “Pré-visualização”:\nJanela de Pré-visualização do Gemini Cloud Assist Se você não estiver satisfeito com as convenções de nomenclatura ou com os detalhes dos componentes gerados, poderá sempre alterá-los clicando no componente e abrindo o painel de configuração. Aqui abri o painel de configuração do meu frontend-service:\nVisão do painel de detalhes do componente Observe que esta tela também mostra qual contêiner é instanciado pelo Cloud Run, que assume como padrão um contêiner “hello”. Isso ocorre porque o Gemini Cloud Assist não tem informações sobre qual contêiner você deseja executar, mas se você fornecer essas informações, ele poderá substituir o valor.\nEstou destacando isso aqui também por outro motivo - precisamos definir as expectativas de que esta ferramenta não codifica o aplicativo para você, ela apenas projeta a infraestrutura para suportá-lo. Para codificar os serviços de frontend e backend reais, por exemplo, você precisará usar outras ferramentas como o Gemini CLI ou seu IDE regular e publicar os artefatos em seu registro de contêiner para que o Cloud Run possa acessá-los.\nNa janela de Pré-visualização, você pode editar componentes, mas não adicionar componentes manualmente. Se você quiser iterar no design, o que você pode fazer é pedir ao Gemini para modificar o design para você. Veja, por exemplo, este prompt de acompanhamento:\nAdicione um serviço de streaming que capture eventos para cada foto de gato gerada. Do outro lado do stream, há um serviço de consumidor que atualizará uma página estática hospedada no GCS, adicionando as fotos mais recentes a um feed.\nEsta é a resposta do Gemini:\nResposta do Gemini ao prompt de acompanhamento E a janela de Pré-visualização será atualizada com o novo design, destacando adições (verde), modificações (azul) e exclusões (vermelho):\nAlterações propostas no diagrama Na parte inferior da tela, você tem a opção de aceitar ou rejeitar a sugestão. Mas antes disso, é uma boa oportunidade para inspecionar o código terraform que é gerado por baixo dos panos. Para ver o código e comparar as alterações, clique em “View diff”:\nIsso abrirá a janela Code Diff com ambas as versões mostradas lado a lado:\nJanela de revisão de diferenças mostrando comparação entre o código terraform antes e depois Como você pode ver, cada caixa no diagrama é mapeada para um módulo terraform diferente. Se você rolar para baixo, poderá ver os módulos que ele adicionou recentemente destacados em verde.\nSe você estiver satisfeito com a implementação, pode aceitar a sugestão ou rejeitar e pedir ao Gemini para melhorá-la. Aceitei a sugestão, mas notei algo um pouco estranho sobre o módulo “database-secrets”, então decidi perguntar ao Gemini sobre isso:\nPrompt: “por que você adicionou um segredo de banco de dados se o banco de dados Cloud SQL está usando autenticação IAM?”\nBem, acho que não era realmente necessário:\nResposta do Gemini à pergunta sobre IAM Na janela de Pré-visualização:\nProposta do Gemini para remover o segredo do banco de dados Este é um alerta importante de que, por mais que a IA tenha se tornado cada vez mais avançada, ainda não estamos isentos de avaliar e tomar decisões. No final das contas, a IA ainda estará lá, mas nossos empregos estão em jogo, então não se esqueça de validar tudo. 🙂\nSobre o tema de validações, outra coisa que me chamou a atenção é que o Gemini estava sugerindo um tipo de instância Cloud SQL razoavelmente grande: db-perf-optimized-N-8. Vamos tentar outro prompt para melhorar isso, pois isso definitivamente é demais para um protótipo pequeno:\nTorne-o econômico\nResposta do Gemini sugerindo um balanceador de carga regional e substituindo o Postgres por MySQL Hmmm… essa me fez pensar. Entendo o ponto sobre o balanceador de carga regional versus global, mas não estou convencido do motivo pelo qual ele acha que o MySQL é mais econômico do que o PostgreSQL. Eu estava mais preocupado com o tipo de máquina do que com a tecnologia de banco de dados real.\nA resposta do Gemini não nos conta toda a história, no entanto. Inspecionando o diff de perto, ele nos mostra que na verdade modificou o tipo de máquina (mostrado como o atributo tier) e simplesmente esqueceu de nos dizer:\nDiff do Terraform mostrando que o Gemini também alterou o tipo de máquina (tier) Não estou totalmente satisfeito com isso, então vou perguntar por quê:\nPerguntando ao Gemini por que ele acha que o MySQL é mais econômico do que o Postgres A resposta sugere que o MySQL é mais econômico que o Postgres devido a:\nDiferenças de licenciamento Consumo de recursos Preços de serviços gerenciados Infelizmente, não posso concordar com esta resposta. Para o item 1, ambos têm licenças de código aberto, portanto, não são tão diferentes. Talvez o item 2 possa ter alguma verdade, mas eu ainda precisaria de um benchmark adequado. O item 3 está errado porque o Cloud SQL para Postgres e MySQL têm o mesmo modelo de preços no GCP. Mais um ponto para os humanos, vamos reverter a mudança:\nreverta a alteração de postgres para mysql, mas mantenha o tipo de máquina menor.\nInspeção final: estou feliz com o Cloud SQL executando Postgres em um nível de banco de dados menor, mas também descobri que há outra edição notável que ativa o recurso de escalonamento para zero do Cloud Run:\nDiff do Terraform mostrando que o Cloud Run foi configurado com escalonamento para zero (min_instance_count = 0) Este faz muito sentido, mas também não foi mencionado no diálogo. Este é outro lembrete para “confiar, mas verificar” o que quer que sua ferramenta de IA esteja lhe dizendo. Não queremos surpresas rodando em produção.\nRecuperando os Arquivos Terraform # Depois de estar satisfeito com o design, você pode clicar no botão “\u0026lt;\u0026gt; Get Code” no canto superior direito da interface do usuário. Isso compactará o código terraform subjacente em um arquivo zip para você baixar para sua máquina local.\nInfelizmente, no momento em que este artigo foi escrito, o Application Design Center não oferece suporte a nenhuma integração com sistemas de controle de versão de código como GitHub, GitLab, Google Source, Bitbucket e outros. A única maneira de extrair o código da ferramenta é por meio do download deste arquivo zip.\nPara pessoas que usam contas corporativas com uma hierarquia organizacional completa, você pode pegar este design e implantá-lo usando o AppHub, mas se você estiver usando sua conta pessoal, infelizmente este é o limite do que a ferramenta pode fazer por você.\nNotas sobre a interface do usuário do App Design Center # O botão “Edit app design” terá comportamentos diferentes dependendo de como seu console da nuvem está configurado. Se você estiver testando este prompt de sua conta pessoal e sua conta pessoal não estiver vinculada a uma organização, ele abrirá uma janela de Pré-visualização onde você poderá ver o design e baixar o código terraform correspondente, mas não terá acesso à interface de usuário completa do App Design Center.\nPara usar a interface completa, você precisa fazer parte de uma organização, pois a configuração do App Design Center precisa de um tipo especial de pasta configurada, denominada pasta “habilitada para o app design center”. Não há como adicionar pastas a contas sem uma organização e, dentro de uma organização, esta pasta precisa ser configurada pelo administrador da nuvem.\nInfelizmente, isso significa que as contas de usuário que não pertencem a nenhuma organização ficarão efetivamente bloqueadas do conjunto completo de recursos do ADC, pelo menos por enquanto.\nVocê ainda poderá usar o Gemini para ajudá-lo a prototipar a arquitetura do seu aplicativo, assim como mostrei neste artigo, mas não poderá salvar seu progresso na interface do usuário da nuvem e precisará baixar os arquivos terraform para sua máquina local e implantá-los usando sua própria instalação do terraform.\nConclusões e próximos passos # Cada novo produto de IA lançado me deixa animado com a ideia de ter aquele momento “Tony Stark” em que você pode projetar seu software apenas usando comandos de voz. Ainda não chegamos lá, mas com o Gemini Cloud Assist estamos progredindo bem, pois agora podemos usar linguagem natural para especificar os componentes de infraestrutura para nós.\nAinda existem algumas arestas ásperas tanto em termos de interface do usuário quanto nas sugestões do Gemini, mas já estou aliviado por não ter que criar manualmente o código terraform para cada novo aplicativo que estou desenvolvendo.\nEste é claramente um artigo que deve ter uma data de validade, pois devemos ver essa ferramenta evoluir muito rapidamente nos próximos meses. Para se manter atualizado, você sempre pode verificar a página do produto Application Design Center, mas é claro que farei o meu melhor para escrever sobre novos recursos e melhorias interessantes neste blog também.\nComo algumas sugestões, recomendo que você experimente alguns prompts criativos como “torne-o econômico”, “torne-o altamente disponível”, “explique por que x em vez de y”, “substitua x por y”, “explique x para mim como se eu tivesse 5 anos”, e assim por diante.\nQuais são seus pensamentos? Você achou esta ferramenta empolgante ou assustadora? Você encontrou algum prompt interessante? Deixe seus comentários abaixo!\n","date":"2 julho 2025","externalUrl":null,"permalink":"/pt-br/posts/20250702-gemini-cloud-assist/","section":"Posts","summary":"Como projetar infraestrutura usando linguagem natural usando o Gemini Cloud Assist no Google Cloud","title":"Do Prompt à Infraestrutura com o Gemini Cloud Assist","type":"posts"},{"content":"","date":"2 julho 2025","externalUrl":null,"permalink":"/pt-br/tags/gemini/","section":"Tags","summary":"","title":"Gemini","type":"tags"},{"content":"","date":"2 julho 2025","externalUrl":null,"permalink":"/pt-br/tags/terraform/","section":"Tags","summary":"","title":"Terraform","type":"tags"},{"content":" Introdução # Neste guia, vamos aprender mais sobre system prompts e ferramentas de agente para que possamos construir uma nova e melhorada experiência de agente de diagnóstico. Trabalharemos com o Vertex AI SDK for Python, LangChain, Gemini e osquery.\nDevo admitir, a versão inicial do agente de diagnóstico não estava muito pronta para a \u0026ldquo;Enterprise\u0026rdquo; (trocadilho intencional). Não tínhamos muita visibilidade sobre o que ele estava fazendo por baixo dos panos (ele estava realmente executando alguma query?), ele não se lembrava de coisas discutidas na mesma \u0026ldquo;sessão\u0026rdquo; e, de vez em quando, também ignorava nossos comandos completamente.\nIsso está longe da experiência que desejamos de um agente adequado. O agente de diagnóstico ideal precisa ser capaz de lembrar seus erros e executar instruções de forma consistente, por exemplo, aprendendo que certas colunas não estão disponíveis e contornando isso. Além disso, podemos realmente confiar que ele está fazendo o que diz que está fazendo? Devemos ser capazes de ver as queries a qualquer momento para garantir que as informações que ele está retornando estão corretas e atualizadas.\nCom esses objetivos em mente, vamos colocar as mãos na massa e começar a construir nosso Agente de Diagnóstico de Emergência Médica Holográfica!\nPreparando o Terreno # Da última vez, escrevemos o código em um Jupyter notebook por conveniência, mas desta vez vamos escrever um programa Python regular. O mesmo código também funcionaria no Jupyter com alterações mínimas, mas estamos fazendo isso para que possamos usar o agente de diagnóstico com uma interface de chat adequada.\nPara qualquer projeto Python, sempre recomendo começar com um virtual env limpo para manter as dependências autocontidas:\n$ mkdir -p ~/projects/diagnostic-agent $ cd ~/projects/diagnostic-agent $ python3 -m venv venv $ source venv/bin/activate $ pip install --upgrade google-cloud-aiplatform[agent_engines,langchain] Aqui está a versão inicial de main.py que reproduz o agente do artigo anterior:\nimport vertexai from vertexai import agent_engines import osquery from rich.console import Console from rich.markdown import Markdown import os PROJECT_ID = os.environ.get(\u0026#34;GCP_PROJECT\u0026#34;) LOCATION = os.environ.get(\u0026#34;GCP_REGION\u0026#34;, \u0026#34;us-central1\u0026#34;) STAGING_BUCKET = os.environ.get(\u0026#34;STAGING_BUCKET_URI\u0026#34;) vertexai.init( project=PROJECT_ID, location=LOCATION, staging_bucket=STAGING_BUCKET ) MODEL = os.environ.get(\u0026#34;GEMINI_MODEL\u0026#34;, \u0026#34;gemini-2.0-flash\u0026#34;) instance = osquery.SpawnInstance() def call_osquery(query: str): \u0026#34;\u0026#34;\u0026#34;Query the operating system using osquery This function is used to send a query to the osquery process to return information about the current machine, operating system and running processes. You can also use this function to query the underlying SQLite database to discover more information about the osquery instance by using system tables like sqlite_master, sqlite_temp_master and virtual tables. Args: query: str A SQL query to one of osquery tables (e.g. \u0026#34;select timestamp from time\u0026#34;) Returns: ExtensionResponse: an osquery response with the status of the request and a response to the query if successful. \u0026#34;\u0026#34;\u0026#34; if not instance.is_running(): instance.open() # This may raise an exception result = instance.client.query(query) return result def get_system_prompt(): if not instance.is_running(): instance.open() # This may raise an exception response = instance.client.query(\u0026#34;select name from sqlite_temp_master\u0026#34;).response tables = [ t[\u0026#34;name\u0026#34;] for t in response ] return f\u0026#34;\u0026#34;\u0026#34; Role: - You are the emergency diagnostic agent. - You are the last resort for the user to diagnose their computer problems. - Answer the user queries to the best of your capabilities. Tools: - you can call osquery using the call_osquery function. Context: - Only use tables from this list: {tables} - You can discover schemas using: PRAGMA table_info(table) Task: - Create a plan for which tables to query to fullfill the user request - Confirm the plan with the user before executing - If a query fails due a wrong column name, run schema discovery and try again - Query the required table(s) - Report the findings in a human readable way (table or list format) \u0026#34;\u0026#34;\u0026#34; def main(): agent = agent_engines.LangchainAgent( model = MODEL, system_instruction=get_system_prompt(), tools=[ call_osquery, ], ) console = Console() print(\u0026#34;Welcome to the Emergency Diagnostic Agent\\n\u0026#34;) print(\u0026#34;What is the nature of your diagnostic emergency?\u0026#34;) while True: try: query = input(\u0026#34;\u0026gt;\u0026gt; \u0026#34;) except EOFError: query = \u0026#34;exit\u0026#34; if query == \u0026#34;exit\u0026#34; or query == \u0026#34;quit\u0026#34;: break if query.strip() == \u0026#34;\u0026#34;: continue response = agent.query(input=query) rendered_markdown = Markdown(response[\u0026#34;output\u0026#34;]) console.print(rendered_markdown) print(\u0026#34;Goodbye!\u0026#34;) if __name__ == \u0026#34;__main__\u0026#34;: main() Você pode executar o agente com python main.py:\n$ python main.py Welcome to the Emergency Diagnostic Agent What is the nature of your diagnostic emergency? \u0026gt;\u0026gt; Existem duas pequenas alterações em comparação com o código original: primeiro, agora temos um loop principal que manterá o agente em execução até que o usuário digite \u0026ldquo;exit\u0026rdquo; ou \u0026ldquo;quit\u0026rdquo;. Isso criará nossa interface de chat.\nSegundo, ajustamos o system prompt para melhorar a consistência do agente. Agora o chamamos de \u0026ldquo;Emergency Diagnostic Agent\u0026rdquo; - este nome não apenas funciona como um elegante [easter egg de Star Trek]https://en.wikipedia.org/wiki/The_Doctor_(Star_Trek:_Voyager), mas também define um tom de urgência que, com base em pesquisas emergentes, pode incentivá-lo a cumprir nossos pedidos com mais diligência. (Confira também esta entrevista recente (original em inglês))\nNão vamos ameaçar nosso pobre Agente de Diagnóstico de Emergência - e posso garantir que nenhum agente foi prejudicado na produção deste texto - mas, chamá-lo de agente de \u0026ldquo;Emergência\u0026rdquo; deve definir o tom para que ele tente cumprir nossos pedidos da melhor maneira possível. Na versão anterior do system prompt, tive casos em que o agente se recusou a fazer uma tarefa porque \u0026ldquo;achava\u0026rdquo; que não era capaz de fazê-la ou não sabia quais tabelas consultar.\nClaro, chamá-lo de agente de emergência não é suficiente para garantir o comportamento desejado, então adicionamos mais algumas instruções para guiar o comportamento do modelo, como veremos abaixo.\nInstruções de Sistema # Instruções de sistema, também chamadas de system prompt, são um conjunto de instruções que guiam o comportamento do LLM durante toda a conversa. As instruções de sistema são especiais, pois têm prioridade mais alta sobre as interações regulares de chat. Uma maneira de imaginar isso é como se as instruções de sistema fossem sempre repetidas junto com o prompt que você está enviando para o modelo.\nNão há um consenso forte na literatura sobre como um system prompt deve ser, mas temos alguns padrões testados em batalha que estão surgindo do uso diário. Por exemplo, um que é praticamente um consenso é dedicar o início do system prompt para atribuir um papel ao agente, para que ele esteja ciente de seu propósito e possa produzir respostas mais coerentes.\nPara este agente em particular, optei por incluir as seguintes seções em meu system prompt: papel, ferramentas, contexto e tarefa. Essa estrutura funcionou bem durante minha fase de testes, mas não se apegue demais a ela: experimente com seus prompts e veja se consegue resultados melhores. A experimentação é fundamental para alcançar bons resultados com LLMs.\nAgora vamos dar uma olhada em cada seção do prompt.\nSystem Prompt: Papel # Um papel nada mais é do que a razão da existência do agente. Pode ser tão simples quanto \u0026ldquo;você é um engenheiro de software\u0026rdquo; ou \u0026ldquo;você é um agente de diagnóstico\u0026rdquo;, mas podem ser um pouco mais elaborados, incluindo uma descrição detalhada, regras de comportamento, restrições e outros.\nPara um modelo de linguagem grande treinado em todos os tipos de dados, o papel ajuda a definir o tom para o conhecimento de domínio que ele precisará acessar para responder às suas perguntas. Em outras palavras, ele dá significado semântico às suas perguntas\u0026hellip; Imagine a pergunta \u0026ldquo;o que são cookies?\u0026rdquo;, por exemplo. Estamos falando de cookies comestíveis ou cookies de navegador? Se o papel do agente for indefinido, essa pergunta é completamente ambígua, mas assim que definimos o papel para algo técnico (por exemplo, \u0026ldquo;você é um engenheiro de software\u0026rdquo;), a ambiguidade desaparece.\nPara este agente, o papel é descrito como:\nRole: - You are the emergency diagnostic agent. - You are the last resort for the user to diagnose their computer problems. - Answer the user queries to the best of your capabilities. Além da definição direta (\u0026ldquo;você é o agente de diagnóstico de emergência\u0026rdquo;), adicionamos uma descrição mais longa para definir o tom do comportamento do modelo e, esperançosamente, influenciá-lo a levar nossos pedidos \u0026ldquo;a sério\u0026rdquo;, como mencionado antes, a iteração anterior deste agente tinha a má tendência de recusar pedidos.\nSystem Prompt: Ferramentas # Ferramentas é a seção que explica ao agente suas capacidades de interagir com sistemas externos além de seu modelo principal. As ferramentas podem ser de vários tipos, mas a maneira mais comum de fornecer uma ferramenta ao agente é por meio de chamadas de função.\nOs agentes podem usar ferramentas para recuperar informações, executar tarefas e manipular dados. O Vertex AI SDK for Python tem suporte para funções fornecidas pelo usuário e ferramentas integradas como Google Search e execução de código. Você também pode usar extensões mantidas pela comunidade por meio da interface Model Context Protocol (MCP).\nPara nosso agente, precisamos dizer a ele que ele pode chamar o osquery:\nTools: - you can call osquery using the call_osquery function. System Prompt: Contexto # Em seguida, temos o contexto, que informa ao agente sobre o ambiente em que opera. Eu uso esta seção para destacar explicitamente e corrigir comportamentos indesejados que iterações anteriores do agente eram propensas a fazer. Por exemplo, percebi bem cedo no desenvolvimento que o agente tentaria \u0026ldquo;adivinhar\u0026rdquo; quais tabelas estavam disponíveis e enviaria queries às cegas, resultando em uma alta taxa de erro. Adicionar a lista de tabelas ao contexto ajudou a mitigar esse problema.\nSemelhante é a tendência do agente de tentar adivinhar os nomes das colunas em uma tabela, em vez de tentar descobrir os nomes primeiro. Neste caso particular, resisti à tentação de instruir o agente a sempre usar SELECT * porque esta é uma má prática (recupera mais dados do que o necessário), mas em vez disso eu o \u0026ldquo;ensinei\u0026rdquo; como descobrir um schema usando a instrução PRAGMA.\nDesta forma, o agente ainda cometerá erros ao adivinhar nomes de colunas, mas tem uma maneira de corrigir o curso sem intervenção humana.\nA seção de contexto revisada do system prompt é mostrada abaixo.\nContext: - Only use tables from this list: {tables} - You can discover schemas using: PRAGMA table_info(table) Note que tables é uma variável que contém todas as tabelas que descobrimos do osquery antes de iniciar o modelo.\nSystem Prompt: Tarefa # Finalmente, a tarefa. Esta seção é usada para descrever como o agente deve interpretar seus pedidos e executá-los. As pessoas geralmente usam esta seção para definir as etapas necessárias para realizar a tarefa em questão.\nNo nosso caso particular, estamos usando esta seção para definir aproximadamente o plano, mas também adicionar algumas diretivas condicionais:\nTask: - Create a plan for which tables to query to fullfill the user request - Confirm the plan with the user before executing - If a query fails due a wrong column name, run schema discovery and try again - Query the required table(s) - Report the findings in a human readable way (table or list format) A etapa \u0026ldquo;confirmar o plano com o usuário antes de executar\u0026rdquo; é interessante, pois nos mostra como o agente está pensando sobre o processo, mas pode ser um pouco irritante depois de interagir com o agente por um tempo. Sempre podemos pedir ao agente para nos dizer o plano com um prompt, então a inclusão desta etapa é totalmente opcional.\nInicialmente pensei nesta etapa como uma forma de depurar o agente, mas na seção seguinte vamos explorar uma maneira diferente de fazer isso.\nCom a combinação dessas quatro seções, temos o system prompt completo. Este prompt revisado produziu resultados mais consistentes durante meus testes em preparação para este artigo. Ele também tem o benefício de ser \u0026ldquo;amigável ao ser humano\u0026rdquo;, por isso é mais fácil de adaptar quando novas regras são introduzidas.\nAqui está a visão completa deste system prompt:\nRole: - You are the emergency diagnostic agent. - You are the last resort for the user to diagnose their computer problems. - Answer the user queries to the best of your capabilities. Tools: - you can call osquery using the call_osquery function. Context: - Only use tables from this list: {tables} - You can discover schemas using: PRAGMA table_info(table) Task: - Create a plan for which tables to query to fullfill the user request - Confirm the plan with the user before executing - If a query fails due a wrong column name, run schema discovery and try again - Query the required table(s) - Report the findings in a human readable way (table or list format) Como nota lateral, acredito que ainda temos muitas oportunidades para melhorá-lo e uma das minhas atuais áreas de interesse é como alcançar um system prompt autoaperfeiçoável. Isso poderia potencialmente ser alcançado pedindo, ao final de uma sessão, para o modelo resumir seus aprendizados em um novo system prompt para sua iteração futura. O prompt poderia ser armazenado em um banco de dados e carregado na próxima sessão. Isso, é claro, levanta preocupações sobre a degradação do system prompt ou, pior ainda, ataques usando injeção de prompt, então não é tão trivial quanto parece. No entanto, é um exercício divertido e posso escrever sobre isso em um futuro próximo.\nHabilitando o Modo de Depuração # Outra preocupação sobre o design original é a falta de observabilidade sobre o que o agente está fazendo por baixo dos panos. Existem duas abordagens diferentes que podemos aplicar aqui, uma um pouco mais dolorosa que a outra: 1) espiar os \u0026ldquo;pensamentos\u0026rdquo; do LLM e tentar encontrar as chamadas de ferramenta entre eles (muito doloroso), ou; 2) adicionar alguma funcionalidade de depuração à própria função para que ela produza as informações que queremos durante a execução (a solução mais fácil geralmente é a correta).\nDevo admitir, passei uma quantidade doentia de tempo na opção 1, antes de perceber que poderia fazer a opção 2. Se você realmente quer seguir o caminho do raciocínio do LLM, pode fazê-lo por meio de uma configuração chamada return_intermediate_steps. Devo dizer que isso é muito interessante do ponto de vista do aprendizado, mas depois de passar algumas horas tentando descobrir o formato da saída (dica: não é exatamente json) decidi que analisar não valia realmente a pena.\nEntão, como funciona a estratégia simples? Estamos adicionando uma flag de depuração e uma ferramenta para ativar e desativar essa flag. Este truque surpreendentemente simples na verdade abre um mundo totalmente novo de potencial: estamos dando ao agente a oportunidade de modificar seu próprio comportamento!\nA implementação do modo de depuração é composta por uma variável global e uma função para configurá-la:\ndebug = False def set_debug_mode(debug_mode: bool): \u0026#34;\u0026#34;\u0026#34;Toggle debug mode. Call this function to enable or disable debug mode. Args: debug_mode (bool): True to enable debug mode, False to disable it. Returns: None \u0026#34;\u0026#34;\u0026#34; global debug debug = debug_mode Também precisamos mencioná-lo no system prompt:\n... Tools: - you can call osquery using the call_osquery function. - you can enable or disable the debug mode using the set_debug_mode function. Context: ... E adicionar a função à lista de ferramentas na instanciação do agente:\nagent = agent_engines.LangchainAgent( model = model, system_instruction=get_system_prompt(), tools=[ call_osquery, set_debug_mode, ], ) Finalmente, precisamos adaptar call_osquery para usar a nova flag debug:\ndef call_osquery(query: str): \u0026#34;\u0026#34;\u0026#34;Query the operating system using osquery This function is used to send a query to the osquery process to return information about the current machine, operating system and running processes. You can also use this function to query the underlying SQLite database to discover more information about the osquery instance by using system tables like sqlite_master, sqlite_temp_master and virtual tables. Args: query: str A SQL query to one of osquery tables (e.g. \u0026#34;select timestamp from time\u0026#34;) Returns: ExtensionResponse: an osquery response with the status of the request and a response to the query if successful. \u0026#34;\u0026#34;\u0026#34; if not instance.is_running(): instance.open() if debug: print(\u0026#34;Executing query: \u0026#34;, query) result = instance.client.query(query) if debug: print(\u0026#34;Query result: \u0026#34;, { \u0026#34;status\u0026#34;: result.status.message if result.status else None, \u0026#34;response\u0026#34;: result.response if result.response else None }) return result Com todas essas alterações implementadas, vamos dar uma olhada em como o agente chama o osquery usando a flag de depuração recém-implementada:\n$ python main.py Welcome to the Emergency Diagnostic Agent What is the nature of your diagnostic emergency? \u0026gt;\u0026gt; run a level 1 diagnostic procedure in debug mode Executing query: SELECT * FROM system_info Query result: {\u0026#39;status\u0026#39;: \u0026#39;OK\u0026#39;, \u0026#39;response\u0026#39;: [{...}]} Executing query: SELECT pid, name, user, cpu_percent FROM processes ORDER BY cpu_percent DESC LIMIT 10 Query result: {\u0026#39;status\u0026#39;: \u0026#39;no such column: user\u0026#39;, \u0026#39;response\u0026#39;: None} Executing query: SELECT pid, name, user, resident_size FROM processes ORDER BY resident_size DESC LIMIT 10 Query result: {\u0026#39;status\u0026#39;: \u0026#39;no such column: user\u0026#39;, \u0026#39;response\u0026#39;: None} Executing query: PRAGMA table_info(processes) Query result: {\u0026#39;status\u0026#39;: \u0026#39;OK\u0026#39;, \u0026#39;response\u0026#39;: [{\u0026#39;cid\u0026#39;: \u0026#39;0\u0026#39;, \u0026#39;dflt_value\u0026#39;: \u0026#39;\u0026#39;, \u0026#39;name\u0026#39;: \u0026#39;pid\u0026#39;, \u0026#39;notnull\u0026#39;: \u0026#39;1\u0026#39;, \u0026#39;pk\u0026#39;: \u0026#39;1\u0026#39;, \u0026#39;type\u0026#39;: \u0026#39;BIGINT\u0026#39;}, ...]} (...) System Information: • Hostname: petruzalek-mac.roam.internal • CPU Type: arm64e • Physical Memory: 51539607552 bytes Top 5 Processes by CPU Usage: PID Name CPU Usage ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1127 mediaanalysisd 95627517 43062 mediaanalysisd-access 66441942 54099 Google Chrome 3005046 54115 Google Chrome Helper (GPU) 2092500 81270 Electron 1688335 Top 5 Processes by Memory Usage (Resident Size): PID Name Resident Size (Bytes) ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 43062 mediaanalysisd-access 3933536256 54099 Google Chrome 1313669120 59194 Code Helper (Plugin) 1109508096 59025 Code Helper (Renderer) 915456000 19681 Google Chrome Helper (Renderer) 736329728 \u0026gt;\u0026gt; Observe que o comando emitido foi \u0026ldquo;run a level 1 diagnostic procedure in debug mode\u0026rdquo;, o que demonstra uma capacidade interessante do agente: invocação de múltiplas ferramentas. Se julgar necessário, ele é capaz de invocar não apenas a mesma ferramenta várias vezes, mas também ferramentas diferentes ao mesmo tempo. Portanto, não foi necessário habilitar o modo de depuração antes de solicitar o relatório: o agente conseguiu fazer tudo de uma vez.\nObserve também como o agente falhou inicialmente ao solicitar uma coluna de usuário, mas depois usou a instrução PRAGMA para descobrir o schema correto e tentar novamente a query com sucesso. Esta é uma demonstração perfeita da capacidade do agente de se recuperar de erros devido ao nosso system prompt aprimorado.\nPreservando o Histórico do Chat # Nossa tarefa final hoje é garantir que o agente se lembre do que estamos falando para que possamos fazer perguntas de esclarecimento e investigar mais o sistema seguindo uma linha de investigação coerente.\nNo artigo anterior exploramos como os LLMs são stateless e que precisamos continuar \u0026ldquo;lembrando-os\u0026rdquo; do estado atual da conversa usando \u0026ldquo;turnos\u0026rdquo;. Felizmente com o LangChain não precisamos fazer isso manualmente e podemos contar com um recurso chamado chat history.\nA beleza do chat history é que qualquer coisa que implemente BaseChatMessageHistory pode ser usada aqui, o que nos permite usar todos os tipos de armazenamentos de dados, incluindo a criação dos nossos próprios. Por exemplo, na documentação oficial do Vertex AI, você pode encontrar exemplos de uso de Firebase, Bigtable e Spanner.\nNão precisamos de um banco de dados completo no momento, então vamos nos contentar com InMemoryChatMessageHistory, que, como o nome sugere, armazenará tudo na memória.\nAqui está uma implementação típica, tecnicamente suportando múltiplas sessões usando o dicionário chats_by_session_id para pesquisa (código retirado da documentação do langchain):\nchats_by_session_id = {} def get_chat_history(session_id: str) -\u0026gt; InMemoryChatMessageHistory: chat_history = chats_by_session_id.get(session_id) if chat_history is None: chat_history = InMemoryChatMessageHistory() chats_by_session_id[session_id] = chat_history return chat_history E aqui está nossa nova função main instanciando o agente com o histórico de chat habilitado:\nimport uuid def main(): session_id = uuid.uuid4() agent = agent_engines.LangchainAgent( model = model, system_instruction=get_system_prompt(), tools=[ call_osquery, set_debug_mode, ], chat_history=get_chat_history, ) Um aviso rápido para que você não cometa o mesmo erro que eu: o argumento chat_history espera um tipo Callable, então você não deve invocar a função ali, mas passar a própria função. O LangChain usa um padrão factory aqui; ele invoca a função fornecida (get_chat_history) sob demanda com um session_id para obter ou criar o objeto de histórico correto. Este design é o que permite ao agente gerenciar múltiplas conversas separadas simultaneamente.\nA assinatura da função pode incluir um ou dois argumentos. Se um argumento, presume-se que seja um session_id, e se forem dois argumentos, eles são interpretados como user_id e conversation_id. Mais informações sobre isso podem ser encontradas na documentação RunnableWithMessageHistory.\nA última peça do quebra-cabeça é passar o session_id para o executor do modelo. Isso é feito por meio do argumento config, conforme mostrado no código abaixo:\n# (...) while True: try: query = input(\u0026#34;\u0026gt;\u0026gt; \u0026#34;) except EOFError: query = \u0026#34;exit\u0026#34; if query == \u0026#34;exit\u0026#34; or query == \u0026#34;quit\u0026#34;: break if query.strip() == \u0026#34;\u0026#34;: continue response = agent.query(input=query, config={\u0026#34;configurable\u0026#34;: {\u0026#34;session_id\u0026#34;: session_id}}) rendered_markdown = Markdown(response[\u0026#34;output\u0026#34;]) console.print(rendered_markdown) Agora, enquanto a sessão estiver ativa, podemos perguntar ao agente sobre informações em sua \u0026ldquo;memória de curto prazo\u0026rdquo;, já que o conteúdo da sessão é armazenado na memória. Isso será suficiente para que a maioria das interações básicas pareçam mais naturais, mas estamos abrindo precedentes para problemas maiores: agora que podemos armazenar informações da sessão, após cada iteração ela só crescerá e, ao lidar com dados gerados automaticamente a partir de queries, o contexto da sessão crescerá muito rapidamente, atingindo em breve os limites do modelo, e muito antes de atingirmos os limites da memória do nosso computador.\nModelos como o Gemini são bem conhecidos por suas longas janelas de contexto, mas mesmo um milhão de tokens podem ser esgotados muito rapidamente se preenchermos o contexto com dados. O contexto longo também pode representar um problema para alguns modelos, pois a recuperação se torna cada vez mais difícil - também conhecido como o problema da agulha no palheiro.\nExistem técnicas para lidar com o problema do contexto crescente, incluindo compressão e sumarização, mas, para manter o contexto deste artigo curto (viu o que eu fiz ali?), vamos guardá-las para o próximo artigo.\nA versão final de main.py, incluindo todas as modificações neste artigo, fica assim:\nimport vertexai from vertexai import agent_engines import osquery from rich.console import Console from rich.markdown import Markdown from langchain_core.chat_history import InMemoryChatMessageHistory import os import uuid PROJECT_ID = os.environ.get(\u0026#34;GCP_PROJECT\u0026#34;) LOCATION = os.environ.get(\u0026#34;GCP_REGION\u0026#34;, \u0026#34;us-central1\u0026#34;) STAGING_BUCKET = os.environ.get(\u0026#34;STAGING_BUCKET_URI\u0026#34;) vertexai.init( project=PROJECT_ID, location=LOCATION, staging_bucket=STAGING_BUCKET ) MODEL = os.environ.get(\u0026#34;GEMINI_MODEL\u0026#34;, \u0026#34;gemini-2.5-pro-preview-05-06\u0026#34;) instance = osquery.SpawnInstance() debug = False def set_debug_mode(debug_mode: bool): \u0026#34;\u0026#34;\u0026#34;Toggle debug mode. Call this function to enable or disable debug mode. Args: debug_mode (bool): True to enable debug mode, False to disable it. Returns: None \u0026#34;\u0026#34;\u0026#34; global debug debug = debug_mode def call_osquery(query: str): \u0026#34;\u0026#34;\u0026#34;Query the operating system using osquery This function is used to send a query to the osquery process to return information about the current machine, operating system and running processes. You can also use this function to query the underlying SQLite database to discover more information about the osquery instance by using system tables like sqlite_master, sqlite_temp_master and virtual tables. Args: query: str A SQL query to one of osquery tables (e.g. \u0026#34;select timestamp from time\u0026#34;) Returns: ExtensionResponse: an osquery response with the status of the request and a response to the query if successful. \u0026#34;\u0026#34;\u0026#34; if not instance.is_running(): instance.open() # This may raise an exception if debug: print(\u0026#34;Executing query: \u0026#34;, query) result = instance.client.query(query) if debug: print(\u0026#34;Query result: \u0026#34;, { \u0026#34;status\u0026#34;: result.status.message if result.status else None, \u0026#34;response\u0026#34;: result.response if result.response else None }) return result def get_system_prompt(): if not instance.is_running(): instance.open() # This may raise an exception response = instance.client.query(\u0026#34;select name from sqlite_temp_master\u0026#34;).response tables = [ t[\u0026#34;name\u0026#34;] for t in response ] return f\u0026#34;\u0026#34;\u0026#34; Role: - You are the emergency diagnostic agent. - You are the last resort for the user to diagnose their computer problems. - Answer the user queries to the best of your capabilities. Tools: - you can call osquery using the call_osquery function. - you can use the set_debug_mode function to enable or disable debug mode. Context: - Only use tables from this list: {tables} - You can discover schemas using: PRAGMA table_info(table) Task: - Create a plan for which tables to query to fullfill the user request - Confirm the plan with the user before executing - If a query fails due a wrong column name, run schema discovery and try again - Query the required table(s) - Report the findings in a human readable way (table or list format) \u0026#34;\u0026#34;\u0026#34; chats_by_session_id = {} def get_chat_history(session_id: str) -\u0026gt; InMemoryChatMessageHistory: chat_history = chats_by_session_id.get(session_id) if chat_history is None: chat_history = InMemoryChatMessageHistory() chats_by_session_id[session_id] = chat_history return chat_history def main(): session_id = uuid.uuid4() agent = agent_engines.LangchainAgent( model = MODEL, system_instruction=get_system_prompt(), tools=[ call_osquery, set_debug_mode ], chat_history=get_chat_history, ) console = Console() print(\u0026#34;Welcome to the Emergency Diagnostic Agent\\n\u0026#34;) print(\u0026#34;What is the nature of your diagnostic emergency?\u0026#34;) while True: try: query = input(\u0026#34;\u0026gt;\u0026gt; \u0026#34;) except EOFError: query = \u0026#34;exit\u0026#34; if query == \u0026#34;exit\u0026#34; or query == \u0026#34;quit\u0026#34;: break if query.strip() == \u0026#34;\u0026#34;: continue response = agent.query(input=query, config={\u0026#34;configurable\u0026#34;: {\u0026#34;session_id\u0026#34;: session_id}}) rendered_markdown = Markdown(response[\u0026#34;output\u0026#34;]) console.print(rendered_markdown) print(\u0026#34;Goodbye!\u0026#34;) if __name__ == \u0026#34;__main__\u0026#34;: main() Conclusões # Neste artigo, aprendemos a importância de ajustar um system prompt para obter respostas consistentes de um agente. Também vimos na prática como funciona a chamada de múltiplas ferramentas e como usar ferramentas para habilitar ou desabilitar flags de recursos para alterar o comportamento do agente. Por último, mas não menos importante, aprendemos sobre como gerenciar o estado da sessão usando o histórico de chat na memória.\nNo próximo artigo da série, veremos como habilitar a persistência entre sessões usando um banco de dados real, revisitaremos a noção de tokens e discutiremos a técnica de compressão de contexto.\nApêndice: Coisas Divertidas para Tentar # Agora que nosso agente está mais robusto, use esta seção como um guia prático para testar os novos recursos em ação. Observe como ele agora se lembra do contexto entre as perguntas e como você pode pedir para ele explicar seu trabalho.\n\u0026gt;\u0026gt; run a level 1 diagnostic procedure \u0026gt;\u0026gt; run a level 2 diagnostic procedure \u0026gt;\u0026gt; explain the previous procedure step by step \u0026gt;\u0026gt; find any orphan processes \u0026gt;\u0026gt; show me the top resource consuming processes \u0026gt;\u0026gt; write a system prompt to transfer your current knowledge to another agent \u0026gt;\u0026gt; search the system for malware \u0026gt;\u0026gt; is this computer connected to the internet? \u0026gt;\u0026gt; why is my computer slow? \u0026gt;\u0026gt; take a snapshot of the current performance metrics \u0026gt;\u0026gt; compare the current perfomance metrics with the previous snapshot \u0026gt;\u0026gt; give me a step by step process to fix the issues you found \u0026gt;\u0026gt; how many osqueryd processes are in memory? \u0026gt;\u0026gt; give me a script to kill all osqueryd processes \u0026gt;\u0026gt; who am i? Se você encontrar outros prompts interessantes, por favor, compartilhe suas experiências na seção de comentários abaixo. Até a próxima!\n","date":"11 junho 2025","externalUrl":null,"permalink":"/pt-br/posts/20250611-system-prompt/","section":"Posts","summary":"Este artigo explora os conceitos de instrução de sistema, histórico de sessão e ferramentas de agente para criar um assistente de diagnóstico mais inteligente.","title":"Prompt Audacioso: Um Guia Prático para Instruções de Sistema e Ferramentas de Agente","type":"posts"},{"content":"","date":"11 junho 2025","externalUrl":null,"permalink":"/pt-br/tags/python/","section":"Tags","summary":"","title":"Python","type":"tags"},{"content":"Daniela Petruzalek é uma profissional de TI experiente com formação em engenharia de software, pré-vendas e relações com desenvolvedores, atualmente Engenheira Sênior de Relações com Desenvolvedores no Google. Sua especialização é engenharia de dados e desenvolvimento back-end e ela é ex-Google Developer Expert em Go e Google Cloud Platform. Ela também é Google Cloud Certified Data Engineer, Oracle Certified Professional e palestrante TEDx. Em seu tempo livre, ela contribui para projetos de código aberto, joga videogames e acaricia gatos aleatórios nas ruas.\nPalestrando em Eventos Se você gostaria de me convidar para palestrar em seu evento, por favor, envie uma mensagem para daniela@danicat.dev com detalhes sobre o evento, incluindo público, local e data.\nVocê pode ver meu conteúdo anterior no GitHub.\nMídias Sociais Atualmente estou mais ativa no LinkedIn, mas sinta-se à vontade para me seguir no X e BlueSky também.\n","date":"11 junho 2025","externalUrl":null,"permalink":"/pt-br/about/","section":"danicat.dev","summary":"","title":"Sobre","type":"page"},{"content":"","date":"11 junho 2025","externalUrl":null,"permalink":"/pt-br/tags/vertex-ai/","section":"Tags","summary":"","title":"Vertex Ai","type":"tags"},{"content":" Introdução # Este artigo explora o modelo de comunicação entre o código do cliente e a API Gemini usando o SDK da Vertex AI para Python. Cobriremos conceitos como a estrutura das mensagens, como o modelo entende o contexto da pergunta e como aumentar as capacidades do modelo com chamadas de função. Embora o Gemini seja o foco deste artigo, os mesmos conceitos que você verá aqui também podem ser aplicados ao Gemma e outros LLMs.\nNo meu post anterior, expliquei como escrever um Agente de IA simples - mas surpreendentemente poderoso - que responde a perguntas de diagnóstico sobre sua máquina local. Em poucas linhas de código (e não tão poucas linhas de comentários), conseguimos fazer nosso agente responder a consultas como “quanta CPU eu tenho na minha máquina” ou “por favor, verifique se há sinais de malware”.\nIsso, claro, deveu-se à beleza do SDK Python, pois simplificou muito as coisas. Por exemplo, contei com um recurso chamado Chamada de Função Automática para permitir que o agente decidisse quando chamar uma função. Esse recurso também me ajudou a definir as funções como funções Python simples e o SDK descobriu sua assinatura e descrição dinamicamente para mim. Essa capacidade, infelizmente, está disponível apenas para o SDK Python, então os desenvolvedores em outras linguagens precisam trabalhar um pouco mais.\nÉ por isso que no artigo de hoje vamos adotar uma abordagem um pouco diferente e discutir como a API Gemini funciona para que você possa estar mais bem preparado para usar não apenas Python, mas qualquer um dos SDKs disponíveis (JS, Go e Java). Continuarei usando Python para os exemplos para que você possa comparar com o artigo anterior, mas os conceitos discutidos aqui são válidos para todas as diferentes linguagens.\nVamos cobrir dois tópicos principais:\nComo funciona a conversa entre cliente e modelo Como implementar chamadas de função manualmente Observe que, se você é um desenvolvedor Python, isso também não significa que não aprenderá nada com este artigo. Na verdade, entender o fluxo da conversa será importante para usar conceitos mais avançados do SDK (como a Live API) e trabalhar com LLMs em geral.\nEntendendo como a API funciona # Os agentes normalmente funcionam da mesma forma que os aplicativos cliente-servidor - você tem um componente cliente responsável por preparar e fazer as solicitações e um processo servidor que hospeda o tempo de execução do modelo e processa as solicitações do cliente.\nPara a Vertex AI, existem dois grupos principais de APIs: uma API REST para o estilo típico de solicitação/resposta de geração de conteúdo, onde o cliente envia uma solicitação e aguarda a resposta antes de continuar, e uma nova Live API que processa informações em tempo real usando websockets. Vamos nos concentrar primeiro nas APIs REST, pois a Live API requer um pouco mais de trabalho preparatório para funcionar corretamente.\nNormalmente, geramos conteúdo em uma das seguintes modalidades: texto, imagem, áudio e vídeo. Muitos dos modelos mais recentes também são multimodais, o que significa que você pode lidar com mais de uma modalidade de entrada e/ou saída ao mesmo tempo. Para simplificar, vamos começar com texto.\nUm aplicativo típico de prompt único se parece com isto:\nfrom google import genai client = genai.Client( vertexai=True, project=\u0026#34;daniela-genai-sandbox\u0026#34;, location=\u0026#34;us-central1\u0026#34; ) response = client.models.generate_content( model=\u0026#34;gemini-2.0-flash\u0026#34;, contents=\u0026#34;How are you today?\u0026#34; ) print(response.text) Saída:\nEstou bem, obrigado por perguntar! Como um modelo de linguagem grande, não experimento emoções como os humanos, mas estou funcionando de forma otimizada e pronto para ajudá-lo. Como posso ajudá-lo hoje? A primeira coisa que precisamos fazer é instanciar o cliente, usando o modo Vertex AI (vertexai=True) ou usando uma chave de API Gemini. Neste caso, estou usando o modo Vertex AI.\nAssim que o cliente é inicializado, podemos enviar-lhe um prompt usando o método client.models.generate_content. Precisamos especificar qual modelo estamos chamando (neste caso gemini-2.0-flash) e o prompt no argumento contents (por exemplo, \u0026quot;How are you today?\u0026quot;).\nOlhando para este código, pode ser difícil imaginar o que está acontecendo por baixo dos panos, pois estamos obtendo muitas abstrações gratuitamente graças ao Python. A coisa mais importante neste caso é que o conteúdo não é uma string.\nContents é na verdade uma lista de estruturas de conteúdo, e as estruturas de conteúdo são compostas por uma função (role) e uma ou mais partes (parts). O tipo subjacente para esta estrutura é definido na biblioteca types e se parece com isto:\nfrom google.genai import types contents = [types.Content( role = \u0026#34;user\u0026#34;, parts = [ types.Part_from_text(\u0026#34;How are you today?\u0026#34;) )] Portanto, sempre que digitamos contents=\u0026quot;How are you today?\u0026quot;, o SDK Python faz essa transformação de string para “conteúdo com uma parte de string” automaticamente para nós.\nOutra coisa importante a notar é que sempre que fazemos uma chamada para generate_content, o modelo está começando do zero. Isso significa que é nossa responsabilidade adicionar o contexto das mensagens anteriores ao próximo prompt. Vamos fazer um teste simples pedindo ao modelo que dia é hoje duas vezes seguidas:\nresponse = client.models.generate_content( model=\u0026#34;gemini-2.0-flash\u0026#34;, contents=\u0026#34;what day is today?\u0026#34; ) print(response.text) response = client.models.generate_content( model=\u0026#34;gemini-2.0-flash\u0026#34;, contents=\u0026#34;what day is today?\u0026#34; ) print(response.text) Saída:\n$ python3 main.py Hoje é domingo, 5 de novembro de 2023. Hoje é sábado, 2 de novembro de 2024. Existem dois problemas com a resposta acima: 1) ela alucinou, pois o modelo não tem como saber a data, e 2) deu duas respostas diferentes para a mesma pergunta. Podemos corrigir o 1) baseando-nos em uma ferramenta como uma chamada de datetime ou Pesquisa Google, mas quero focar no 2) porque mostra claramente que o modelo não se lembra do que acabou de dizer e demonstra o ponto acima de que é nossa responsabilidade manter o modelo atualizado sobre a conversa.\nVamos fazer uma pequena modificação no código:\nresponse = client.models.generate_content( model=\u0026#34;gemini-2.0-flash\u0026#34;, contents=\u0026#34;what day is today?\u0026#34; ) print(response.text) # cada elemento no array de conteúdos é geralmente referido como um \u0026#34;turno\u0026#34; contents = [ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;parts\u0026#34;: [{ \u0026#34;text\u0026#34;: \u0026#34;what day is today?\u0026#34; }] }, { \u0026#34;role\u0026#34;: \u0026#34;model\u0026#34;, \u0026#34;parts\u0026#34;: [{ \u0026#34;text\u0026#34;: response.text }] }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;parts\u0026#34;: [{ \u0026#34;text\u0026#34;: \u0026#34;what day is today?\u0026#34; }] }, ] response = client.models.generate_content( model=\u0026#34;gemini-2.0-flash\u0026#34;, contents=contents ) print(response.text) Saída:\n$ python3 main.py Hoje é quarta-feira, 15 de novembro de 2023. Hoje é quarta-feira, 15 de novembro de 2023. Observe que na segunda chamada ao modelo estamos incluindo todo o contexto no atributo contents. Observe também que a role de cada parte muda de “user” para “model” e depois para “user” novamente (“user” e “model” são os únicos valores possíveis para role). É assim que o modelo entende em que ponto da conversa está, também conhecido como “turno”. Se, por exemplo, omitíssemos a última parte que repete a pergunta, o modelo pensaria que está atualizado e não produziria outra resposta, pois o último turno seria de “model” e não de “user”.\nA variável contents acima está escrita na forma de “dicionário”, mas o SDK também fornece vários métodos de conveniência como types.UserContent (define o campo role como “user” automaticamente) e types.Part.from_text (converte uma string simples em uma parte), entre outros.\nPara lidar com outros tipos de entradas e/ou saídas, podemos usar outros tipos de partes, como chamadas de função, dados binários, etc. Se um modelo for multimodal, você pode misturar partes de diferentes tipos de conteúdo na mesma mensagem.\nOs dados binários podem ser tanto inline quanto buscados de um URI. Você pode diferenciar entre diferentes tipos de dados usando o campo mime_type. Por exemplo, uma parte de imagem pode ser recuperada assim:\nfrom google.genai import types contents = types.Part.from_uri( file_uri: \u0026#39;gs://generativeai-downloads/images/scones.jpg\u0026#39;, mime_type: \u0026#39;image/jpeg\u0026#39;, ) Ou inline:\ncontents = types.Part.from_bytes( data: my_cat_picture, # dados binários mime_type: \u0026#39;image/jpeg\u0026#39;, ) Em resumo, para cada turno da conversa, adicionaremos uma nova linha de conteúdo tanto para a resposta anterior do modelo quanto para a nova pergunta do usuário.\nA boa notícia é que a experiência de chatbot é um caso de uso tão importante que o SDK da Vertex AI fornece uma implementação para esse fluxo pronta para uso. Usando o recurso chat, podemos reproduzir o comportamento acima em poucas linhas de código:\nchat = client.chats.create(model=\u0026#39;gemini-2.0-flash\u0026#39;) response = chat.send_message(\u0026#39;what day is today?\u0026#39;) print(response.text) response = chat.send_message(\u0026#39;what day is today?\u0026#39;) print(response.text) Saída:\n$ python3 main.py Hoje é sábado, 14 de outubro de 2023. Hoje é sábado, 14 de outubro de 2023. Desta vez, o modelo lembrou a data porque a interface de chat está lidando com o histórico automaticamente para nós.\nChamada de função não automática # Agora que vimos como a API funciona para construir mensagens do cliente e gerenciar o contexto, é hora de explorar como ela lida com chamadas de função. Em um nível básico, precisaremos instruir o modelo de que ele tem uma função à sua disposição e, em seguida, processar suas solicitações para chamar a função e retornar os valores resultantes ao modelo. Isso é importante porque as chamadas de função permitem que os agentes interajam com sistemas externos e o mundo real, criando ações como recuperar dados ou acionar processos específicos, indo além de apenas gerar texto.\nA declaração da função é o que diz ao modelo o que ele pode fazer. Ela informa ao modelo o nome da função, a descrição e seus argumentos. Por exemplo, abaixo está uma declaração de função para a função get_random_number:\nget_random_number_decl = { \u0026#34;name\u0026#34;: \u0026#34;get_random_number\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Retorna um número aleatório\u0026#34;, } É essa declaração que o modelo precisa saber para decidir quais funções chamar. A declaração da função tem três campos: nome, descrição e parâmetros - neste caso, a função não aceita parâmetros, então este campo é omitido. O modelo usa a descrição da função e a descrição de seus argumentos para decidir quando e como chamar cada função.\nNo artigo anterior, em vez de dar ao modelo uma declaração de função, fui preguiçoso e deixei o SDK descobrir isso para mim com base no docstring da minha função. Desta vez, vamos fazer diferente e declarar explicitamente uma função para entender melhor o fluxo subjacente.\nA função, incluindo sua declaração, se parece com isto:\ndef get_random_number(): return 4 # escolhido por um lançamento de dado justo # garantido ser aleatório (https://xkcd.com/221/) # a declaração informa ao modelo o que ele precisa saber sobre a função get_random_number_decl = { \u0026#34;name\u0026#34;: \u0026#34;get_random_number\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Retorna um número aleatório\u0026#34;, } Você pode ver outros exemplos de declarações de função aqui.\nEm seguida, precisamos dizer ao modelo que ele tem acesso a esta função. Fazemos isso por meio da configuração do modelo, adicionando a função como uma ferramenta.\ntools = types.Tool(function_declarations=[get_random_number_decl]) config = types.GenerateContentConfig(tools=[tools]) # meu prompt inicial contents = [types.Part.from_text(text=\u0026#34;what is my lucky number today?\u0026#34;)] response = client.models.generate_content( model=\u0026#34;gemini-2.0-flash\u0026#34;, contents=contents, config=config, # observe como estamos adicionando a configuração à chamada do modelo ) print(response.candidates[0].content.parts[0]) Se você executar o código acima, obterá algo assim:\n$ python3 main.py video_metadata=None thought=None inline_data=None file_data=None thought_signature=None code_execution_result=None executable_code=None function_call=FunctionCall(id=None, args={}, name=\u0026#39;get_random_number\u0026#39;) function_response=None text=None O que você está vendo aqui é a primeira parte da resposta do modelo, e podemos ver que esta parte tem todos os campos vazios (None), exceto o campo function_call. Isso significa que o modelo quer que nós façamos essa chamada de função e, em seguida, retornemos seu resultado de volta ao modelo.\nIsso inicialmente me intrigou, mas se você pensar bem, faz todo o sentido. O modelo sabe que a função existe, mas não tem absolutamente nenhuma ideia de como chamá-la. Da perspectiva do modelo, a função também não está rodando na mesma máquina, então o modelo não pode fazer nada exceto “pedir educadamente” para que façamos a chamada em seu nome.\nNão tivemos que fazer isso no meu artigo anterior porque a Chamada de Função Automática assumiu o controle e simplificou as coisas para nós. A chamada ainda seguiu o mesmo fluxo, mas o SDK escondeu toda essa complexidade de nós.\nA coisa óbvia a fazer agora é chamar a função real e retornar o resultado para o modelo, mas lembre-se, sem contexto o modelo não sabe nada sobre nossa solicitação anterior, então se você enviar apenas os resultados da função de volta, ele não terá ideia do que fazer com isso!\nÉ por isso que precisamos enviar o histórico da interação até agora, e pelo menos até o ponto em que o modelo sabe que solicitou esse valor. O código abaixo assume que recebemos uma mensagem de chamada de função e precisamos enviar uma nova solicitação com as informações completas:\n# assumindo que já inspecionamos a resposta e sabemos o que o modelo quer result = get_random_number() # faz a chamada de função real # contents ainda contém o prompt original, então adicionaremos a resposta do modelo... contents.append(types.ModelContent(parts=response.candidates[0].content.parts)) # ... e o resultado da chamada de função contents.append(types.UserContent(parts=types.Part.from_function_response(name=\u0026#34;get_random_number\u0026#34;, response={\u0026#34;result\u0026#34;: result}))) response = client.models.generate_content( model=\u0026#34;gemini-2.0-flash\u0026#34;, contents=contents, config=config, ) print(response.text) Saída:\n$ python3 main.py O número da sorte de hoje é 4. Conclusões # Neste artigo, vimos como o cliente do agente se comunica com o modelo no lado do servidor ou, em outras palavras, o “modelo de domínio” das comunicações LLM. Também removemos a cortina da “mágica” que o SDK Python faz por nós.\nA automação é sempre conveniente e nos ajuda a alcançar resultados muito mais rapidamente, mas saber como ela realmente funciona geralmente é a grande diferença entre uma jornada tranquila e uma irregular ao implementar seu próprio agente, especialmente porque os casos especiais nunca são tão fáceis.\nEu sei que em tempos de \u0026ldquo;vibe coding\u0026rdquo;, à primeira vista, é quase irônico dizer algo assim, mas uma das coisas que aprendi rapidamente ao programar no \u0026ldquo;vibe coding\u0026rdquo; é que se você for mais preciso ao falar com a IA, obterá resultados muito melhores em muito menos tempo. Portanto, agora não é hora de menosprezar o valor do conhecimento, mas sim de dobrá-lo - não apesar da IA, mas por causa dela.\nEspero que você tenha gostado da jornada até agora. No próximo artigo, construiremos sobre este conhecimento para levar o agente de diagnóstico ao próximo nível, onde nenhum agente jamais esteve! (ou talvez tenha estado, mas certamente não o meu =^_^=)\nPor favor, escreva seus comentários abaixo! Paz o/\n","date":"5 junho 2025","externalUrl":null,"permalink":"/pt-br/posts/20250605-vertex-ai-sdk-python/","section":"Posts","summary":"Este artigo explora o modelo de comunicação entre o código do cliente e a API Gemini usando o SDK da Vertex AI para Python","title":"Aprofundando-se no SDK da Vertex AI para Python","type":"posts"},{"content":"Espaço: a fronteira final. Estas são as viagens da nave estelar Enterprise. Sua missão de 5 anos: explorar novos mundos estranhos; procurar novas vidas e novas civilizações; audaciosamente ir onde nenhum homem jamais esteve.\nIntrodução # Enquanto crescia, graças à influência do meu pai, acostumei-me a ouvir essas palavras quase todos os dias. Suspeito que a paixão dele por Star Trek desempenhou um papel enorme na minha escolha pela carreira de engenharia de software. (Para aqueles que não estão familiarizados com Star Trek, este discurso era reproduzido no início de cada episódio da série original de Star Trek)\nStar Trek sempre esteve à frente de seu tempo. Mostrou o primeiro beijo inter-racial na televisão dos EUA, em tempos em que tal cena causava muita controvérsia. Também retratou muitas peças de tecnologia “futurista” que hoje são commodities, como smartphones e videoconferência.\nUma coisa realmente notável é como os engenheiros da série interagem com os computadores. Embora vejamos alguns teclados e pressionamentos de botões de vez em quando, muitos dos comandos são vocalizados em linguagem natural. Alguns dos comandos que eles dão ao computador são bastante icônicos, como por exemplo, quando solicitam ao computador para executar um “procedimento de diagnóstico de nível 1”, o que aconteceu tantas vezes que praticamente se tornou uma piada entre os fãs mais assíduos.\nAvançando mais de 30 anos e aqui estamos nós, na Era da IA, uma revolução tecnológica que promete ser maior que a internet. Claro que muitas pessoas estão com medo de como a IA pode impactar seus empregos (escrevi sobre isso na semana passada), mas crescer assistindo Star Trek torna mais fácil para mim ver como o papel do engenheiro mudará nos próximos anos. Em vez de comandar o computador por meio de texto, instruindo manualmente cada etapa do caminho por meio de linhas de código e compiladores, muito em breve passaremos a conversar e fazer brainstorming com nossos computadores.\nPara ajudar as pessoas a visualizar isso, vamos usar a tecnologia que temos hoje para criar um pequeno agente que nos permite interagir com nossas próprias máquinas usando linguagem natural.\nO que você precisará para esta demonstração # Para a linguagem de desenvolvimento, usaremos Python em um Jupyter Notebook, pois ele funciona muito bem para experimentação. As principais ferramentas e bibliotecas que usaremos são:\nVertex AI Agent Engine Osquery com bindings para Python Jupyter Notebook [opcional] (na verdade, estou usando o plugin Jupyter para VSCode) Os exemplos abaixo usarão o Gemini Flash 2.0, mas você pode usar qualquer variante do modelo Gemini. Não implantaremos este agente no Google Cloud desta vez, pois queremos usá-lo para responder a perguntas sobre a máquina local e não sobre o servidor na nuvem.\nVisão Geral do Agente # Se você já está familiarizado com o funcionamento da tecnologia de agentes, pode pular esta seção.\nUm agente de IA é uma forma de IA capaz de perceber seu ambiente e tomar ações autônomas para atingir objetivos específicos. Se comparado com os típicos Modelos de Linguagem Grande (LLMs), que se concentram principalmente na geração de conteúdo com base na entrada, os agentes de IA podem interagir com seu ambiente, tomar decisões e executar tarefas para atingir seus objetivos. Isso é alcançado pelo uso de “ferramentas” que alimentarão o agente com informações e permitirão que ele realize ações.\nPara demonstrar a tecnologia de agente, usaremos o LangChain por meio do Agent Engine. Primeiro, você precisa instalar os pacotes necessários em seu sistema:\npip install --upgrade --quiet google-cloud-aiplatform[agent_engines,langchain] Você também precisará definir suas credenciais padrão de aplicativo (ADC) do gcloud:\ngcloud auth application-default login Nota: dependendo do ambiente que você está usando para executar esta demonstração, pode ser necessário usar um método de autenticação diferente.\nAgora estamos prontos para trabalhar em nosso script Python. Primeiro, vamos inicializar o SDK com base no ID e local do nosso projeto do Google Cloud:\nimport vertexai vertexai.init( project=\u0026#34;my-project-id\u0026#34;, # Seu ID de projeto. location=\u0026#34;us-central1\u0026#34;, # Sua localização na nuvem. staging_bucket=\u0026#34;gs://my-staging-bucket\u0026#34;, # Seu bucket de preparo. ) Uma vez feita a configuração inicial, criar um agente usando LangChain no Agent Engine é bastante simples:\nfrom vertexai import agent_engines model = \u0026#34;gemini-2.0-flash\u0026#34; # sinta-se à vontade para experimentar diferentes modelos! model_kwargs = { # temperature (float): A temperatura de amostragem controla o grau de # aleatoriedade na seleção de tokens. \u0026#34;temperature\u0026#34;: 0.20, } agent = agent_engines.LangchainAgent( model=model, # Obrigatório. model_kwargs=model_kwargs, # Opcional. ) A configuração acima é suficiente para você enviar consultas ao agente, assim como enviaria uma consulta a um LLM:\nresponse = agent.query( input=\u0026#34;which time is now?\u0026#34; ) print(response) O que poderia retornar algo assim:\n{\u0026#39;input\u0026#39;: \u0026#39;which time is now?\u0026#39;, \u0026#39;output\u0026#39;: \u0026#39;Como uma IA, eu não tenho uma hora ou local \u0026#34;atuais\u0026#34; da mesma forma que um humano. Meu conhecimento não é atualizado em tempo real.\\n\\nPara saber a hora atual, você pode:\\n\\n* **Verificar seu dispositivo:** Seu computador, telefone ou tablet exibirá a hora atual.\\n* **Fazer uma pesquisa rápida:** Digite \u0026#34;que horas são\u0026#34; em um mecanismo de busca como o Google.\u0026#39;} Dependendo de suas configurações, prompt e da aleatoriedade do universo, o modelo pode lhe dar uma resposta dizendo que não pode lhe dizer a hora, ou pode “alucinar” e inventar um timestamp. Mas, na verdade, como a IA não tem relógio, ela não será capaz de responder a essa pergunta\u0026hellip; a menos que você lhe dê um relógio!\nChamadas de Função # Uma das maneiras mais convenientes de estender as capacidades do nosso agente é dar-lhe funções Python para chamar. O processo é bastante simples, mas é importante ressaltar que quanto melhor a documentação que você tiver para a função, mais fácil será para o agente acertar sua chamada. Vamos definir nossa função para verificar a hora:\nimport datetime def get_current_time(): \u0026#34;\u0026#34;\u0026#34;Retorna a hora atual como um objeto datetime. Args: Nenhum Returns: datetime: hora atual como um tipo datetime \u0026#34;\u0026#34;\u0026#34; return datetime.datetime.now() Agora que temos uma função que nos dá a hora do sistema, vamos recriar o agente, mas agora ciente de que a função existe:\nagent = agent_engines.LangchainAgent( model=model, # Obrigatório. model_kwargs=model_kwargs, # Opcional. tools=[get_current_time] ) E faça a pergunta novamente:\nresponse = agent.query( input=\u0026#34;which time is now?\u0026#34; ) print(response) A saída será semelhante a esta:\n{\u0026#39;input\u0026#39;: \u0026#39;which time is now?\u0026#39;, \u0026#39;output\u0026#39;: \u0026#39;A hora atual é 18:36:42 UTC de 30 de maio de 2025.\u0026#39;} Agora o agente pode contar com a ferramenta para responder à pergunta com dados reais. Muito legal, hein?\nColetando Informações do Sistema # Para nosso agente de diagnóstico, vamos dar a ele recursos para consultar informações sobre a máquina em que está sendo executado usando uma ferramenta chamada osquery. Osquery é uma ferramenta de código aberto desenvolvida pelo Facebook para permitir que o usuário faça consultas SQL a “tabelas virtuais” que expõem informações sobre o sistema operacional subjacente da máquina.\nIsso é conveniente para nós porque não apenas nos dá um único ponto de entrada para fazer consultas sobre o sistema, mas os LLMs também são muito proficientes em escrever consultas SQL.\nVocê pode encontrar instruções sobre como instalar o osquery na documentação oficial. Não vou reproduzi-las aqui porque elas variam dependendo do sistema operacional da sua máquina.\nDepois de instalar o osquery, você precisará instalar os bindings Python para o osquery. Como é típico em Python, é apenas um pip install:\npip install --upgrade --quiet osquery Com os bindings instalados, você pode fazer chamadas ao osquery importando o pacote osquery:\nimport osquery # Inicia um processo osquery usando um soquete de extensão efêmero. instance = osquery.SpawnInstance() instance.open() # Isso pode levantar uma exceção # Emite consultas e chama APIs Thrift do osquery. instance.client.query(\u0026#34;select timestamp from time\u0026#34;) O método query retornará um objeto ExtensionResponse com os resultados de sua consulta. Por exemplo:\nExtensionResponse(status=ExtensionStatus(code=0, message=\u0026#39;OK\u0026#39;, uuid=0), response=[{\u0026#39;timestamp\u0026#39;: \u0026#39;Sex Mai 30 17:54:06 2025 UTC\u0026#39;}]) Se você nunca trabalhou com o osquery antes, encorajo você a dar uma olhada no schema para ver que tipo de informação está disponível em seu sistema operacional.\nUma nota lateral sobre formatação # Todas as saídas dos exemplos anteriores não foram formatadas, mas se você estiver executando o código do Jupyter, poderá acessar alguns métodos de conveniência para embelezar a saída importando os seguintes pacotes:\nfrom IPython.display import Markdown, display E exibindo a saída da resposta como markdown:\nresponse = agent.query( input=\u0026#34;what is today\u0026#39;s stardate?\u0026#34; ) display(Markdown(response[\u0026#34;output\u0026#34;])) Saída:\nDiário do Capitão, Suplementar. A data estelar atual é 48972.5. Conectando os pontos # Agora que temos uma maneira de consultar informações sobre o sistema operacional, vamos combinar isso com nosso conhecimento de agentes para criar um agente de diagnóstico que responderá a perguntas sobre nosso sistema.\nO primeiro passo é definir uma função para fazer as consultas. Isso será dado ao agente como uma ferramenta para coletar informações posteriormente:\ndef call_osquery(query: str): \u0026#34;\u0026#34;\u0026#34;Consulta o sistema operacional usando osquery Esta função é usada para enviar uma consulta ao processo osquery para retornar informações sobre a máquina atual, sistema operacional e processos em execução. Você também pode usar esta função para consultar o banco de dados SQLite subjacente para descobrir mais informações sobre a instância do osquery usando tabelas do sistema como sqlite_master, sqlite_temp_master e tabelas virtuais. Args: query: str Uma consulta SQL para uma das tabelas do osquery (por exemplo, \u0026#34;select timestamp from time\u0026#34;) Returns: ExtensionResponse: uma resposta do osquery com o status da solicitação e uma resposta à consulta, se bem-sucedida. \u0026#34;\u0026#34;\u0026#34; return instance.client.query(query) A função em si é bastante trivial, mas a parte importante aqui é ter um docstring bem detalhado que permitirá ao agente entender como essa função funciona.\nDurante meus testes, um problema complicado que ocorreu com bastante frequência foi que o agente não sabia exatamente quais tabelas estavam disponíveis em meu sistema. Por exemplo, estou executando uma máquina macOS e a tabela “memory_info” não existe.\nPara dar ao agente um pouco mais de contexto, vamos fornecer dinamicamente os nomes das tabelas que estão disponíveis neste sistema. Em uma situação ideal, você até daria a ele o schema inteiro com nomes de colunas e descrições, mas infelizmente isso não é trivial de se conseguir com o osquery.\nA tecnologia de banco de dados subjacente para o osquery é o SQLite, então podemos consultar a lista de tabelas virtuais da tabela sqlite_temp_master:\n# use um pouco de mágica Python para descobrir quais tabelas temos neste sistema response = instance.client.query(\u0026#34;select name from sqlite_temp_master\u0026#34;).response tables = [ t[\u0026#34;name\u0026#34;] for t in response ] Agora que temos todos os nomes das tabelas, podemos criar o agente com esta informação e a ferramenta call_osquery:\nosagent = agent_engines.LangchainAgent( model = model, system_instruction=f\u0026#34;\u0026#34;\u0026#34; Você é um agente que responde a perguntas sobre a máquina em que está sendo executado. Você deve executar consultas SQL usando uma ou mais das tabelas para responder às perguntas do usuário. Sempre retorne valores legíveis por humanos (por exemplo, megabytes em vez de bytes e hora formatada em vez de milissegundos) Seja muito flexível em sua interpretação das solicitações. Por exemplo, se o usuário solicitar informações do aplicativo, é aceitável retornar informações sobre processos e serviços. Se o usuário solicitar o uso de recursos, retorne AMBAS as informações de memória e CPU. Não peça esclarecimentos ao usuário. Você tem as seguintes tabelas disponíveis para você: ----- TABELAS ----- {tables} ----- FIM TABELAS ----- Pergunta: \u0026#34;\u0026#34;\u0026#34;, tools=[ call_osquery, ] ) Acredite ou não, nosso agente está pronto para todas as nossas perguntas! Vamos experimentar:\nresponse = osagent.query(input=\u0026#34;what is the current time?\u0026#34;) display(Markdown(response[\u0026#34;output\u0026#34;])) Saída:\nA hora atual é Sex Mai 30 18:08:15 2025 UTC. Vamos complicar um pouco mais:\nresponse = osagent.query(input=\u0026#34;what is the top consuming process?\u0026#34;) display(Markdown(response[\u0026#34;output\u0026#34;])) Saída:\nO processo que mais consome CPU é: Nome do Processo: mediaanalysisd, PID: 1127, Tempo Total de CPU: 2876716 O processo que mais consome memória é: Nome do Processo: Code Helper (Plugin), PID: 2987, Uso de Memória: 1537 MB E se eu for um pouco mais criativo?\nresponse = osagent.query(input=\u0026#34;computer, run a level 1 diagnostic procedure\u0026#34;) display(Markdown(response[\u0026#34;output\u0026#34;])) Saída:\nAqui estão as informações de diagnóstico de nível 1: **Sistema Operacional:** * **Nome:** macOS * **Versão:** 15.5 * **Build:** 24F74 * **Plataforma:** darwin * **Arquitetura:** arm64 ... omitido por questões de brevidade ... **Tempo de Atividade:** * **Segundos Totais:** 261164 * **Dias:** 3 * **Horas:** 0 * **Minutos:** 32 * **Segundos:** 44 Uau! Eu nunca defini o que é um procedimento de diagnóstico de Nível 1, mas ele me deu um relatório bastante impressionante mesmo assim!\nAté tentei ser um pouco mais esperto nas minhas perguntas e as respostas não decepcionaram (na maioria das vezes):\nresponse = osagent.query(input=\u0026#34;computer, do you see any signs of malware running?\u0026#34;) display(Markdown(response[\u0026#34;output\u0026#34;])) Saída:\nVerifiquei se há processos que não estão associados a um arquivo em disco, o que pode ser um sinal de malware, e não encontrei nenhum. Também examinei os principais processos por uso de memória e CPU. Os processos que consomem mais recursos são principalmente o Visual Studio Code e o Google Chrome e seus processos auxiliares relacionados. Este é um comportamento típico para esses aplicativos. Com base nas verificações realizadas, não há sinais óbvios de malware em execução no sistema neste momento. Mic drop =^.^=\nConclusões # Eu sei que já é um argumento batido, mas a IA é um divisor de águas. Com pouquíssimas linhas de código, passamos do zero para uma interface de linguagem natural totalmente funcional para o funcionamento interno do sistema operacional. Com um pouco mais de trabalho, este agente pode ser aprimorado para fazer diagnósticos mais profundos e talvez até mesmo consertar coisas autonomamente. Scotty ficaria orgulhoso!\nVocê pode encontrar o código-fonte de todos os exemplos deste artigo no meu GitHub.\nQuais são suas impressões? Compartilhe suas ideias nos comentários abaixo.\n","date":"31 maio 2025","externalUrl":null,"permalink":"/pt-br/posts/20250531-diagnostic-agent/","section":"Posts","summary":"Como criar um agente de diagnóstico que fala linguagem natural usando Gemini e Vertex AI Agent Engine","title":"Como transformei meu computador na \"USS Enterprise\" usando Agentes de IA","type":"posts"},{"content":"","date":"28 maio 2025","externalUrl":null,"permalink":"/pt-br/tags/opiniao/","section":"Tags","summary":"","title":"Opiniao","type":"tags"},{"content":"","date":"28 maio 2025","externalUrl":null,"permalink":"/tags/opinion/","section":"Tags","summary":"","title":"Opinion","type":"tags"},{"content":" Introdução # Você acredita que um não engenheiro pode programar um aplicativo pronto para produção no \u0026ldquo;vibe code\u0026rdquo;? Esta é a pergunta sobre a qual tenho refletido ultimamente.\nO cenário da IA está se movendo tão rapidamente, e estamos vendo tantas ferramentas legais sendo lançadas quase todos os dias, então é natural que muitas pessoas estejam ficando com medo e começando a \u0026ldquo;prever\u0026rdquo; o fim da carreira de engenharia de software. Veja o Jules por exemplo, um agente de IA que pode automatizar muitas tarefas típicas de engenheiros, como atualizar dependências, escrever documentação, refatorar e até mesmo testar.\nSe essas são as únicas coisas que você está agregando como engenheiro, talvez sim, você deva ter medo. Mas o medo não é necessariamente algo ruim, ele nos tira da zona de conforto, nos permite correr riscos que de outra forma não correríamos. Uma boa dose de medo impulsiona o autodesenvolvimento e a inovação.\nAfinal, o que é um Engenheiro? # Talvez eu não seja a melhor pessoa para responder a essa pergunta porque, embora eu tenha estudado Engenharia Elétrica na faculdade, na verdade nunca me formei. No entanto, posso dizer que, mesmo sem diploma, o período que passei no curso foi fundamental para moldar minha mentalidade ao abordar problemas em minha carreira.\nUm dos momentos cruciais que me influenciaram foi uma masterclass com o Dr. Ewaldo Mehl (UFPR) discutindo a carreira de engenharia para nossa turma de calouros. Entre muitas observações brilhantes, uma que ficou comigo até agora - mais de 20 anos depois - foi sobre a natureza do trabalho que fazemos\u0026hellip; Ele disse algo como: \u0026ldquo;Engenheiros são os profissionais mais próximos dos deuses (\u0026hellip;) porque não apenas consertamos coisas, nós as criamos.\u0026rdquo;\nSim, eu sei que este comentário fora de contexto pode parecer estranho, mas o ponto principal da conversa estava relacionado ao \u0026ldquo;complexo de Deus\u0026rdquo; com o qual algumas carreiras como a Medicina frequentemente lutam, e meu professor estava tirando sarro disso dizendo que os engenheiros não deveriam se sentir inferiores aos médicos porque, na verdade, somos nós que deveríamos ter esse complexo. (Por favor, mantenham os comentários em ordem, isso foi no início dos anos 2000, então estávamos muito próximos dos anos 90 :-)\nÉ um ditado comum de onde eu venho que no fundo de toda piada há um pouco de verdade, então, deixando as coisas de lado, acho que o Dr. Mehl estava certo em algo\u0026hellip; Removendo todas as ferramentas, em nossa essência, somos criadores. A maneira como criamos evolui. Costumava ser com nossas próprias mãos e matérias-primas. Depois vieram ferramentas, computadores e robôs. Agora, estamos entrando na era da IA.\nEntão, sempre que você pensar sobre a carreira de engenharia - e se ela vai morrer ou não - pense na necessidade humana de criar coisas novas. Sempre estaremos iterando, melhorando e inovando. Esta é uma característica tão fundamental da natureza humana que não consigo acreditar que vá desaparecer, seja nos próximos 5 anos ou no próximo milênio.\nMas a maneira como criamos e melhoramos as coisas, isso sim, acredito que vai mudar.\nOrquestrando, Não Apenas Codificando # Acho que a revolução da IA nos libertará das partes chatas para nos concentrarmos no aspecto mais empolgante do nosso trabalho: arquitetura, design, planejamento e teste das soluções.\nNão estou dizendo que codificar em si é chato. Na verdade, muito pelo contrário, como a maioria dos engenheiros, gosto bastante de codificar. Mas manter código profissional é diferente de escrever código por diversão. Há tantas coisas que você escreverá que não se traduzem totalmente nas coisas que você deseja alcançar.\nPense em todos os não funcionais, como por exemplo segurança, logging, métricas e networking; as horas gastas configurando, implantando, testando e validando; todas as etapas de configuração para integrar diferentes camadas de aplicação, bancos de dados e assim por diante\u0026hellip; Muitas das coisas que fazemos no dia a dia são apenas para \u0026ldquo;fazer funcionar\u0026rdquo;, em oposição a realmente resolver um problema de negócio.\nMesmo tarefas simples, como configurar este blog, ainda me levaram alguns dias de trabalho - tive que configurar meu ambiente de desenvolvimento, pesquisar qual é a melhor ferramenta para blogs em 2025, pesquisar opções de hospedagem, pesquisar a linguagem de domínio usada para descrever blogs na minha plataforma escolhida (por exemplo, como os sites Hugo são organizados), testar alguns templates antes de selecionar um, pesquisar como adicionar comentários às postagens\u0026hellip; e assim por diante.\nComo tudo isso se relaciona com o problema de negócio que quero resolver? São etapas importantes, mas só as fiz porque são etapas necessárias para \u0026ldquo;fazer funcionar\u0026rdquo;, e não realmente o que eu queria alcançar em primeiro lugar.\nO que eu realmente quero é criar uma fonte de verdade que eu possa usar para publicar posts de blog e depois transmiti-los para diferentes plataformas para melhor alcance (por exemplo, Medium, LinkedIn, etc.). Ainda não consegui resolver este problema de negócio porque tive que me concentrar em todos os aspectos do pipeline de desenvolvimento de um novo aplicativo, desde o início até a produção.\nQuão bom seria se pudéssemos abstrair tudo isso e apenas pedir a uma IA para \u0026ldquo;gerar uma plataforma de blog para mim, com tudo incluído\u0026rdquo;?\nÉ aqui que a IA se tornará rapidamente muito poderosa. Com o nascimento da IA Agêntica, em breve teremos milhares de agentes à nossa disposição para orquestrar todas as etapas do processo de desenvolvimento de software, deixando-nos com a tarefa principal de ideação de novas experiências, e a \u0026ldquo;operacionalização\u0026rdquo; disso será responsabilidade dos agentes. Eles irão codificar, implantar e iterar. Eles farão a pesquisa por nós e nos darão opções, para que possamos escolher de acordo com nossos ideais.\nPode ser difícil para você imaginar como isso funcionaria sem um exemplo concreto, mas é por isso que amo ficção científica, pois nos permite dar uma espiada no futuro. Pense no computador de Star Trek\u0026hellip; Acredito que em alguns anos estaremos interagindo com o computador da mesma forma que Geordy La Forge faz nesta cena - o processo de engenharia se torna um diálogo mais do que um esforço unilateral de um engenheiro.\nJá chegamos lá? Na verdade não, mas a IA Agêntica é o próximo passo para tornar realidade o futuro retratado em Star Trek. Definitivamente, está começando a parecer que a IA é maior que a internet, e como desenvolvedores, devemos nos esforçar para nos atualizarmos para a \u0026ldquo;próxima geração\u0026rdquo; (trocadilho intencional).\nEntão, Qualquer Um Pode Programar no \u0026ldquo;Vibe Code\u0026rdquo;? # Talvez não qualquer um, mas a barreira de entrada está definitivamente ficando mais baixa. As habilidades essenciais estão evoluindo, mas o papel do engenheiro de criar e arquitetar ainda está aqui e estará aqui por muito tempo.\nO que você acha? Você consegue se ver como um engenheiro mais \u0026ldquo;hands-off\u0026rdquo;, guiando a IA para dar vida às suas visões? Vamos discutir nos comentários!\n","date":"28 maio 2025","externalUrl":null,"permalink":"/pt-br/posts/20250528-vibe-coding/","section":"Posts","summary":"Uma reflexão sobre o futuro da carreira de engenharia de software.","title":"Qualquer um pode programar no \"vibe code\"?","type":"posts"},{"content":"Olá a todos, vamos falar sobre o Jules! Saído direto do forno do Google I/O, isto é o que o Google chama de um agente de codificação autônomo… mas o que é um agente de codificação autônomo? Pense no NotebookLM, mas para codificação - uma IA especializada para ajudá-lo com tarefas de programação. A principal diferença da abordagem tradicional de “vibe coding” é que com o Jules você pode importar todo o seu projeto como contexto para a IA, então todas as respostas são baseadas no código em que você está realmente trabalhando!\nDepois que o projeto é importado, você pode interagir com o Jules enviando “tarefas”, que podem ser qualquer coisa, desde correções de bugs, atualizações de dependência, novos recursos, planejamento, documentação, testes e assim por diante. Assim que recebe uma tarefa, o Jules planejará assincronamente sua execução em etapas e realizará diferentes subtarefas para garantir que o resultado desejado seja alcançado. Por exemplo, garantindo que nenhum teste foi quebrado pela nova alteração.\nEle se integra diretamente com o GitHub, então há muito pouco atrito para começar a usá-lo. Ele ainda não substituirá completamente o IDE, mas você pode realizar muitas tarefas diretamente do Jules até o ponto em que ele cria um branch com todas as alterações solicitadas, pronto para ser transformado em um pull request.\nA consequência infeliz do anúncio do Jules ontem é que a ferramenta está atualmente sob forte carga, então pode levar um tempo depois que você envia uma tarefa para ver os resultados, mas o Jules fará o trabalho em segundo plano e se você tiver as notificações do navegador ativadas, ele o avisará quando estiver pronto.\nDiante disso, não consegui fazer grandes experimentos com ele, mas uma das coisas que fiz foi gerar o README para o projeto do meu blog no Github (a fonte desta mesma página que você está lendo agora). Também tentei algumas iterações mais complexas, como ajustar o template do blog. Ele gerou os arquivos corretos, mas demorou um pouco para responder às minhas solicitações, então tive que fazer algumas alterações manualmente.\nNada mal para o primeiro dia, eu diria, e há muito potencial a ser desbloqueado nas próximas semanas e meses. O recurso matador é a capacidade de trabalhar em uma base de código completa, em vez daquele fluxo tradicional de fazer uma pergunta ao Gemini (ou ChatGPT), copiar o código-fonte para o IDE, executar, copiar e colar de volta os resultados no LLM e iterar. Claro, ferramentas como Code Assist e CoPilot fornecerão algumas dessas capacidades sem sair do IDE, mas ainda sinto que o IDE não é o ambiente certo para o vibe coding, pois parece mais um hack.\nNesse espírito, talvez o Jules seja a injeção de inspiração que precisávamos para uma nova era de IDEs que desbloqueará o potencial da IA para desenvolvedores em todo o mundo de uma forma mais natural. Pelo menos é o que espero!\nO Jules está atualmente em beta público e você pode brincar com ele hoje inscrevendo-se em https://jules.google.\n","date":"21 maio 2025","externalUrl":null,"permalink":"/pt-br/posts/20250521-jules/","section":"Posts","summary":"O novo agente de codificação autônomo que todo desenvolvedor precisa conhecer.","title":"Precisamos falar sobre o Jules!","type":"posts"},{"content":"Bem-vindo ao danicat.dev! Sou Daniela Petruzalek, engenheira de software com mais de 20 anos de experiência na área e atualmente trabalho como Engenheira de Relações com Desenvolvedores no Google. Este é o meu novo blog pessoal, onde vou compartilhar os últimos desenvolvimentos em tecnologia!\nSe você já está familiarizado com o meu conteúdo da época em que eu era Google Developer Expert, talvez saiba o que esperar, mas caso contrário, confira meu conteúdo anterior no meu repositório do GitHub de apresentações públicas!\nEstou sempre aberta a colaborações e palestras. Se você quiser me ver no seu próximo evento, ou colaborar comigo em um vídeo ou podcast, por favor, entre em contato comigo no LinkedIn ou envie uma mensagem para daniela@danicat.dev\nPor enquanto é só, mas fique ligado para conteúdo sobre engenharia de software, Google Cloud, GenAI, engenharia de dados e muito mais!\nDani =^.^=\nP.S.: Por favor, note que as opiniões escritas neste blog são minhas e não representam as opiniões do meu empregador! ^^\n","date":"20 maio 2025","externalUrl":null,"permalink":"/pt-br/posts/20250520-hello-world/","section":"Posts","summary":"Apenas algumas palavras para dar o pontapé inicial neste blog!","title":"Olá Mundo","type":"posts"},{"content":"","externalUrl":null,"permalink":"/pt-br/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":"","externalUrl":null,"permalink":"/pt-br/series/","section":"Series","summary":"","title":"Series","type":"series"}]