


[{"content":"Hello and welcome! I\u0026rsquo;m Daniela Petruzalek and this is my personal blog. I\u0026rsquo;m a technologist with experience in backend and data engineering, and I\u0026rsquo;m currently a Developer Relations Engineer at Google. In this blog I talk about technology, best practices, career and sometimes cats. You can check more about my profile in the About page.\nDisclaimer: the views written on this blog are my own and do not necessarily represent the views of my employer.\n","date":"7 July 2025","externalUrl":null,"permalink":"/","section":"danicat.dev","summary":"A blog for the lovers of technology, cats and coffee =^.^=","title":"danicat.dev","type":"page"},{"content":"","date":"7 July 2025","externalUrl":null,"permalink":"/events/","section":"Events","summary":"","title":"Events","type":"events"},{"content":"","date":"7 July 2025","externalUrl":null,"permalink":"/events/20250806-gophercon-south-africa/","section":"Events","summary":"","title":"Gophercon South Africa","type":"events"},{"content":" Description # GopherCon UK is an annual event with two multi-track conference days and one workshop day, held in the Brewery, in the heart of London. Three days of amazing talks, plentiful networking opportunities and great socials. GopherCon UK offers the most up-to-date Go programming information and training.\nSession Details # Specific session details for 2025 are not yet available. Please check the event website for updates.\n","date":"7 July 2025","externalUrl":null,"permalink":"/events/20250813-gophercon-uk/","section":"Events","summary":"","title":"Gophercon UK","type":"events"},{"content":"","date":"7 July 2025","externalUrl":null,"permalink":"/events/20250917-tdc-sao-paulo/","section":"Events","summary":"","title":"TDC Sao Paulo 2025","type":"events"},{"content":" Description # The World\u0026rsquo;s Leading Event for Developers, AI Innovators \u0026amp; Tech Leaders. Join the largest gathering of software innovators, tech leaders, and decision-makers shaping the future of AI-powered technology.\nSession Details # Specific session details for 2025 are not yet available. Please check the event website for updates.\n","date":"7 July 2025","externalUrl":null,"permalink":"/events/20250709-wearedevelopers-world-congress/","section":"Events","summary":"","title":"WeAreDevelopers World Congress 2025","type":"events"},{"content":"","date":"3 July 2025","externalUrl":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":" Author\u0026rsquo;s note: about 90% of this blog post is written by AI, but I proofread and edited it to ensure the post read nicely. It was funny how Jules had the tendency to brag about itself. I had to guide it with many prompts to get to this final result, but the last edit was easier to do manually. You can see the full edit history on the PR commit history. A notable mention is that it completely refused to translate this post to Portuguese (Brazil) stating that it doesn\u0026rsquo;t have translation capabilities, but the entirety of this blog was translated using Jules in a previous interaction. I guess it was not in the mood. :)\nIntroduction # I recently decided to update the homepage of my blog to better highlight the most recent content. As a backend engineer, diving deep into frontend intricacies isn\u0026rsquo;t my usual day-to-day, so instead of manually coding all the changes in a less familiar domain, I enlisted the help of Jules, an AI coding assistant.\nThis post details our iterative journey, the successes, the (sometimes amusing) misunderstandings, and what I learned about working effectively with AI for web development, especially in bridging skill gaps.\nThe Goal: A Featured Post Section # My initial request to Jules was straightforward:\n\u0026ldquo;Change the layout of the main page so that it displays the most recent blog post in highlight instead of it being in the recent posts list. The recent posts should contain all other posts except the most recent one. This behaviour should be seen only on the blog landing page (home). If the user clicks on the Blog menu it should still see all the posts in reverse chronological order, including the most recent one.\u0026rdquo;\nJules quickly understood and proposed a plan involving exploring the Hugo codebase, identifying templates, and modifying them.\nIteration Highlights: The Good, The Bad, and The AI # Our collaboration involved several iterations to get things just right.\nIteration 1: Initial Setup - Getting the Basics Right # Jules correctly identified the Blowfish theme\u0026rsquo;s partials and set up the override structure. The logic to separate the latest post from the others in the \u0026ldquo;Recent Posts\u0026rdquo; list was implemented well.\nWhat worked: Understanding the core Hugo structure, fetching posts, basic template modifications. Jules\u0026rsquo;s ability to navigate the theme and project files was a significant time-saver here. There was a lot of awaiting in between tasks Iteration 2: Styling with Tailwind - The Trial-and-Error Dance # We then focused on the appearance: title, width, and image dimensions. This involved a series of prompts to fine-tune the visuals. For example:\n\u0026ldquo;Change the featured post title to \u0026lsquo;Featured Post\u0026rsquo;. Adjust its width to be about 80% of the view. The image is too tall/narrow, let\u0026rsquo;s try a 4:3 aspect ratio. That\u0026rsquo;s still not quite right, make it wider/less tall.\u0026rdquo;\nThis is where the iterative nature of working with Jules on visual elements became very apparent.\nJules\u0026rsquo;s approach: Modified i18n files for titles, used various Tailwind width classes (e.g., md:w-4/5, md:w-2/3, max-w-xl, max-w-2xl), and manipulated padding-bottom for image aspect ratios.\nChallenge \u0026amp; Frustration: A particular challenge, especially for someone like me who primarily works on the backend, was the trial-and-error nature of styling with Tailwind through an intermediary. While Jules could apply the classes it thought appropriate, the visual outcome wasn\u0026rsquo;t always immediately what I had in mind. Changes in Tailwind classes often didn\u0026rsquo;t translate to a clearly visible difference on the first try, or the effect was not as expected. This led to a few rounds of \u0026ldquo;try this class,\u0026rdquo; \u0026ldquo;no, make it narrower/wider/taller/shorter,\u0026rdquo; which, while ultimately successful, could feel frustrating at times. It highlighted the disconnect between code and immediate visual feedback in this AI-assisted async workflow.\nLearning: Fine-tuning visual aesthetics is the least favourite part of my experience, as the instructions will often result to false positives. Clear, descriptive feedback is key, but also recognizing that some back-and-forth is inevitable when I can\u0026rsquo;t directly point at a screen or make micro-adjustments myself in real-time. Jules, however, diligently applied each requested change, which helped bridge my frontend knowledge gap.\nIteration 3: Custom CSS vs. Tailwind - A Brief Detour # At one point, to get very specific control over the card\u0026rsquo;s dimensions, I prompted:\n\u0026ldquo;jules, instead of trying to use an existing style class, create an unique style class for the featured post card. This style should use relative width and height of 75% of the container\u0026hellip;\u0026rdquo;\nJules\u0026rsquo;s response: Jules correctly created the custom CSS rules and refactored the card\u0026rsquo;s partial to use them.\nOutcome \u0026amp; Learning: While Jules implemented this as requested, the result felt somewhat alien to the rest of the blog\u0026rsquo;s design, which is heavily Tailwind-based. The custom CSS didn\u0026rsquo;t quite harmonize, and I quickly decided that maintaining consistency with Tailwind was more important. This was a good lesson in ensuring that even AI-generated solutions fit the existing design language and my preference for sticking to the established framework. Jules adapted back to Tailwind upon request:\n\u0026ldquo;undo the last change and restore the tailwind style of formatting. apply the same style guidelines using tailwind best practices\u0026rdquo;\nIteration 4: The Great \u0026ldquo;Comments\u0026rdquo; Misunderstanding! # This was perhaps the most illustrative part of the AI-human interaction. I mentioned:\n\u0026ldquo;the comments are rendering in the featured post. please remove all the comments or make them invisible\u0026rdquo;\nJules\u0026rsquo;s interpretation: Jules assumed I meant the blog\u0026rsquo;s user comment system (like Utterances or Giscus) or metadata like views/likes counts. This led to a series of steps where Jules tried to investigate and then conditionally hide views/likes metadata. My Clarification: After these changes, I clarified by giving it an example: \u0026ldquo;you are wrong, I never said I wanted to remove the views and likes - I\u0026rsquo;m referring to the code comments in rendering as {/* Adjusted padding \u0026hellip; /} and {/ Removed prose classes \u0026hellip; */}\u0026rdquo;\nResolution: Once Jules understood I meant literal Go template/HTML comments that were incorrectly formatted (using {/*...*} which isn\u0026rsquo;t a valid Hugo comment style and thus renders as text) and not {{/* ... */}}, the fix was immediate: remove the offending text from the templates. What worked: Jules\u0026rsquo;s persistence and systematic approach to debugging the (misunderstood) problem was commendable. Challenge \u0026amp; Learning: This highlighted a crucial aspect of AI interaction: ambiguity in natural language. \u0026ldquo;Comments\u0026rdquo; has multiple meanings. My initial report wasn\u0026rsquo;t precise enough. Iteration 5: Final Polish # After resolving the visible template comments, we made final tweaks:\n\u0026ldquo;Remove the \u0026lsquo;Featured Post\u0026rsquo; title. Change card width to 50%. Increase title and summary font sizes. Make the image\u0026rsquo;s aspect ratio 16:9.\u0026rdquo;\nThis led to the final adjustments for the card\u0026rsquo;s width, font sizes, and image aspect ratio. The width % didn\u0026rsquo;t have any effect, but changing the aspect ratio did the trick.\nBonus Iteration: Jules Drafts This Blog Post # \u0026ldquo;This is perfect. No more code changes are needed. Now I want you to create a new blog post entry describing the iteration we just did\u0026hellip;\u0026rdquo;\nAnd here we are! This post itself was drafted with the assistance of Jules, based on our interaction log and my guiding feedback, including the very points you\u0026rsquo;re reading now.\nWhat Worked Well with Jules # Bridging Skill Gaps: As a backend engineer, Jules was invaluable in tackling frontend tasks involving Hugo templating and Tailwind CSS, areas where I have less day-to-day experience. Jules made up for my lack of deep frontend knowledge, proposing and implementing solutions that I could then guide and refine. Speed of Implementation: For well-understood changes, Jules can modify code, create files, and refactor structures much faster than manual typing. Handling Complex Instructions: Generally, Jules understood multi-step requests and complex layout goals. Systematic Problem Solving: Even with misunderstandings, Jules often followed a logical process. Iterative Refinement: Jules was consistently receptive to feedback for tweaks. Challenges and Learnings # Precision of Language: The \u0026ldquo;comments\u0026rdquo; incident underscores how critical precise language is. What\u0026rsquo;s obvious to a human, or shorthand, might be ambiguous to an AI. Visual Feedback Loop \u0026amp; Tailwind: The trial-and-error with Tailwind styling was a key challenge. Without Jules \u0026ldquo;seeing\u0026rdquo; the output, describing desired visual outcomes or why a particular set of classes wasn\u0026rsquo;t working as hoped required patience and detailed descriptions. This is inherent in text-based interaction for visual tasks. Misinterpretation \u0026amp; Course Correction: When Jules misunderstood a task, it would proceed diligently down that incorrect path. There wasn\u0026rsquo;t a way to interrupt it mid-task; I had to wait for it to complete its current action sequence before providing corrective feedback. Asynchronous Workflow \u0026amp; Pace: The work is mostly asynchronous. Each request and Jules\u0026rsquo;s implementation could take from a few minutes to sometimes half an hour for more complex sequences. This makes the iterative loop slower than direct coding with instant feedback or live pair programming. Suggested Resources # For those interested in learning more about Jules:\nJules Official Website Jules Documentation Conclusion # Overall, working with Jules on this homepage feature was a productive experience. It truly felt like \u0026ldquo;vibe-coding\u0026rdquo; – a dynamic exchange guiding the AI. The key to success lies in clear, iterative communication, patience during misunderstandings, and a willingness to provide specific, actionable feedback.\nThe frustrations, particularly with the Tailwind trial-and-error and the AI\u0026rsquo;s occasional misinterpretations, are part of the current landscape of AI-assisted development. However, despite the asynchronous nature and the time taken for some generations, the ability to offload the mechanical aspects of coding and to get suggestions for areas outside my core expertise (like specific Tailwind implementations or Hugo structures) still resulted in a net gain. It was significantly faster and more effective than if I had attempted to learn all the necessary frontend design principles, Hugo intricacies, and Tailwind CSS nuances from scratch for this specific feature.\nAI assistants like Jules are powerful tools. They don\u0026rsquo;t replace human oversight or design intent, but they can be incredible accelerators when approached with the right mindset and communication strategy.\n","date":"3 July 2025","externalUrl":null,"permalink":"/posts/20250703-jules-featured-post/","section":"Posts","summary":"A detailed account of my iterative process working with Jules, an AI coding assistant, to implement a new featured post section on my blog\u0026rsquo;s homepage.","title":"How I used Jules to add a featured post to this blog","type":"posts"},{"content":"","date":"3 July 2025","externalUrl":null,"permalink":"/categories/interaction-logs/","section":"Categories","summary":"","title":"Interaction Logs","type":"categories"},{"content":"","date":"3 July 2025","externalUrl":null,"permalink":"/tags/jules/","section":"Tags","summary":"","title":"Jules","type":"tags"},{"content":"","date":"3 July 2025","externalUrl":null,"permalink":"/posts/","section":"Posts","summary":"","title":"Posts","type":"posts"},{"content":"","date":"3 July 2025","externalUrl":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"},{"content":"","date":"3 July 2025","externalUrl":null,"permalink":"/tags/vibe-coding/","section":"Tags","summary":"","title":"Vibe-Coding","type":"tags"},{"content":"","date":"2 July 2025","externalUrl":null,"permalink":"/tags/cloud-assist/","section":"Tags","summary":"","title":"Cloud Assist","type":"tags"},{"content":" Introduction # Today we are going to take a small detour from our usual AI agent content to talk about a product I’ve come to explore recently as part of my participation at I/O Connect Berlin 2025 last week.\nThis event brought together over 1000 developers from all over Europe, including members from the Google developer communities (Google Developer Groups) and community experts. It was also my first official Google event since I joined the DevRel team back in April, so it was particularly meaningful for me - and this is why we didn’t get a blog update last week!\nI was responsible for a demo called “Design and Deploy” which showcases the combination of two products: Application Design Center (ADC) and Gemini Cloud Assist (GCA). The demo was so well received that I thought it would be nice to bring this content to the blog as well to give the opportunity to the people who were not there to play with this technology as well.\nApplication Design Center is a product to help architects and developers to design their application infrastructure. On the front it provides a nice user interface where you can visually define components for your infrastructure, but under the hood everything in the UI is represented as a terraform module so you can also leverage the benefits of Infrastructure as Code.\nAn important disclaimer is that ADC is currently in public preview. This means that the product is evolving every day and sometimes it might break compatibility with previous iterations. It also has notably a few rough edges that I’m going to mention below, which should be addressed before the product becomes generally available.\nGemini Cloud Assist (also on public preview), on the other hand, is the official product name for the Gemini support in Google Cloud. Because of this, GCA is not a standalone product, but more like a connective tissue that enables users to interact with anything GCP using natural language, including all the benefits of the modern chatbot experience based on large language models.\nLet’s see how we can use both of these technologies to quickly design the infrastructure part of an application for us.\nHow to start an application design session # You can always open the Application Design Center manually from the Google Cloud console, but what is the fun in that? The best way to trigger ADC for a new design is to simply open the Gemini panel in any page. Here, for example, I’m using the Welcome page of my project:\nWelcome screen at the Google Cloud console If you click on the “star” button on the right side of the search bar you are going to open the Gemini Cloud Assist pane:\nZoomed in view of the Gemini button Should open:\nGoogle Cloud Assist welcome screen This is the panel where you can interact with Gemini. Type something like “create an application that does x” and include as many details as you would like about the architecture. For example, let’s try creating an application that generates cat pictures. Here is the prompt:\nCreate an application that generates cat pictures with Gemini and stores them in a Cloud SQL database. Users can request new pictures using a generation service and can see the generated pictures with a pictures service. Both services are exposed through a frontend service and a global load balancer.\nAfter entering the prompt, Gemini will think for a while and after a few seconds produce an output like this:\nGemini response with architecture diagram The built-in visualisation gives us an idea, but we can better interact with the design if we click on the “Edit app design” button. This will open the design in an expanded view so we can further refine it. (Please note that the remainder of this article assumes the “Edit app design” button opens the Preview window. If in your case it doesn`t, check the notes at the bottom of the article)\nThis is how it looks on the “Preview” window:\nGemini Cloud Assist Preview window If you are not happy with the naming conventions or with the details of the generated components you can always change them by clicking on the component and opening the configuration panel. Here I opened the configuration panel from my frontend-service:\nView of the component details panel Note that this screen also shows which container is instantiated by Cloud Run, which defaults to a “hello” container. This is because Gemini Cloud Assist doesn’t have information about which container you want to run, but if you provide this information it will be able to replace the value.\nI’m highlighting this here also for another reason - we need to set the expectations that this tool doesn’t actually code the application for you, it only designs the infrastructure to support it. For coding the actual frontend and backend services, for example, you will need to use other tools like the Gemini CLI or your regular IDE and publish the artifacts to your container registry so that Cloud Run can access them.\nIn the Preview window you can edit components, but not add components manually. If you want to iterate on the design, what you can do is to ask Gemini to modify the design for you. Check for example this follow up prompt:\nAdd a streaming service that captures events for every cat picture generated. On the other side of the stream there is a consumer service that will update a static page hosted on GCS, adding the most recent pictures to a feed.\nThis is Gemini’s response:\nGemini response to follow up prompt And the Preview window will be updated with the new design, highlighting additions (green), modifications (blue) and deletions (red):\nProposed diagram changes At the bottom of the screen you are given the option to accept or reject the suggestion. But before that, it is a good opportunity to inspect the terraform code that is generated under the hood. To see the code and compare the changes, click on “View diff”:\nThis will open the Code Diff window with both versions shown side by side:\nReview diff window showing comparison between terraform code before and after As you can see, each box in the diagram is mapped to a different terraform module. If you scroll down to the bottom you can see the modules it recently added highlighted in green.\nIf you are happy with the implementation you can accept the suggestion or reject and ask Gemini to improve it. I accepted the suggestion, but I noticed something slightly weird about the “database-secrets” module, so I decided to ask Gemini about it:\nPrompt: “why did you add a database secret if the Cloud SQL database is using IAM authentication?”\nOh well, I guess it was not really necessary:\nGemini response to IAM question On the Preview window:\nGemini\u0026#39;s proposal to remove the database secret This is an important call out that as much as AI has become more and more advanced, we are still not exempt from evaluating and making decisions. At the end of the day, the AI will still be there, but our jobs are on the line, so don’t forget to validate everything. 🙂\nOn the topic of validations, another thing that caught my attention is that Gemini was suggesting a fairly big Cloud SQL instance type: db-perf-optimized-N-8. Let’s try another prompt to improve this, as this definitely is too much for a small prototype:\nMake it cost effective\nGemini\u0026#39;s response suggesting a regional load balancer and replacing Postgres with MySQL Hmmm… this one got me thinking. I can see the point on regional vs global load balancer, but I’m not sold on why it thinks MySQL is more cost-effective than PostgreSQL. I was more concerned about the machine type than the actual database technology.\nGemini’s answer doesn’t tell us the whole story either. Inspecting the diff closely it shows us that it actually modified the machine type (shown as the attribute tier) and just forgot to tell us:\nTerraform diff showing that Gemini also changed the machine type (tier) I’m not entirely satisfied with this, so I’m going to ask why:\nWhy do you consider MySQL more cost effective than PostgreSQL?\nAsking Gemini why it things MySQL is more cost-effective than Postgres The response suggests that MySQL is more cost effective than Postgres due to:\nLicensing differences Resource consumption Managed service pricing Unfortunately I cannot agree with this answer. For item 1 both have open source licenses so they are not that different. Maybe item 2 might have some truth to it but I would still need a proper benchmark. Item 3 is wrong because Cloud SQL for Postgres and MySQL have the same pricing model on GCP. One more point for humans, let’s revert the change:\nrevert the change from postgres to mysql, but keep the smaller machine type.\nFinal inspection: I\u0026rsquo;m happy with Cloud SQL running Postgres on a smaller database tier, but I also found there is another notable edit that enables Cloud Run scale to zero feature:\nTerraform diff showing that Cloud Run was configured with scale to zero (min_instance_count = 0) This one makes a lot of sense, but it was also not mentioned in the dialogue. This is another reminder to “trust but verify” whatever your AI tooling is telling you. We don’t want any surprises running in production.\nRetrieving the Terraform Files # Once you are happy with the design you can click on the “\u0026lt;\u0026gt; Get Code” button on the top-right corner of the UI. This will pack the underlying terraform code into a zip file for you to download to your local machine.\nUnfortunately at the time of this writing, the Application Design Center doesn’t support any integrations with code versioning systems like GitHub, GitLab, Google Source, Bitbucket and others. The only way to extract the code from the tool is through this zip file download.\nFor people who are using corporate accounts with a full organization hierarchy, you can take this design and deploy it using AppHub, but if you are using your personal account unfortunately this is the limit of what the tool can do for you.\nNotes on the App Design Center UI # The “Edit app design” button will have different behaviors depending on how your cloud console is set up. If you are testing this prompt from your personal account and your personal account is not attached to an organization, it will open a Preview window where you can see the design and download the corresponding terraform code, but you won’t have access to the full user interface for App Design Center.\nTo use the full interface you need to be part of an organization, as the App Design Center setup needs an special kind of folder configured refered as an “app design center enabled” folder. There is no way to add folders to accounts without an organization and within an organization this folder needs to be set up by the cloud administrator.\nUnfortunately, this means that user accounts that do not belong to any organizations will be effectively locked out of the full set of ADC features, at least for the time being.\nYou will still be able to use Gemini to help you prototype your app architecture just like I’ve shown in this article, but you won’t be able to save your progress on the Cloud UI and will need to download the terraform files to your local machine and deploy it using your own terraform installation.\nConclusions and next steps # Every new AI product that is released makes me excited about the idea of having that “Tony Stark” moment where you can design your software just using voice commands. We are not there yet, but with Gemini Cloud Assist we are making good progress as now we can use natural language to specify the infrastructure components for us.\nThere are still a few rough edges both in terms of the UI and Gemini’s suggestions, but I’m already relieved that I don’t have to handcraft terraform code for every new application I’m developing.\nThis is clearly one article that should have an expiration date, as we should see this tooling evolve very quickly over the next couple of months. To keep yourself up to date, you can always check the Application Design Center product page, but of course I will do my best to write about interesting new features and improvements on this blog as well.\nAs a few suggestions, I would recommend you to try some creative prompts like “make it cost effective”, “make it highly available”, “explain why x instead of y”, “replace x with y”, “explain x to me like I’m 5”, and so on.\nWhat are your thoughts? Did you find this tool exciting or scary? Have you found any interesting prompts? Leave your comments below!\n","date":"2 July 2025","externalUrl":null,"permalink":"/posts/20250702-gemini-cloud-assist/","section":"Posts","summary":"How to design infrastructure using natural language on Google Cloud using Gemini Cloud Assist","title":"From Prompt to Infrastructure with Gemini Cloud Assist","type":"posts"},{"content":"","date":"2 July 2025","externalUrl":null,"permalink":"/tags/gemini/","section":"Tags","summary":"","title":"Gemini","type":"tags"},{"content":"","date":"2 July 2025","externalUrl":null,"permalink":"/tags/terraform/","section":"Tags","summary":"","title":"Terraform","type":"tags"},{"content":"Daniela Petruzalek is an experienced IT professional with background in software engineering, pre-sales and developer relations, currently a Senior Developer Relations Engineer at Google. Her specialisation is data engineering and back end development and she is a former Google Developer Expert in Go and Google Cloud Platform. She\u0026rsquo;s also a Google Cloud Certified Data Engineer, Oracle Certified Professional and TEDx speaker. In her spare time she contributes to open source, plays video games and pets random cats on the streets.\nSpeaking at Events If you would like to invite me to speak at your event please drop me a message at daniela@danicat.dev with details about the event including audience, location and date.\nYou can see my previous content on GitHub.\nSocial Media I\u0026rsquo;m currently more active on LinkedIn, but feel free to follow me on X and BlueSky as well.\n","date":"11 June 2025","externalUrl":null,"permalink":"/about/","section":"danicat.dev","summary":"","title":"About","type":"page"},{"content":" Introduction # In this guide, we are going to learn more about system prompts and agent tools so that we can build a new and improved diagnostic agent experience. We will be working with the Vertex AI SDK for Python, LangChain, Gemini and osquery.\nI must admit, the initial version of the diagnostic agent was not very “Enterprise” ready (pun intended). We didn’t have much visibility on what it was doing under the hood (was it actually running any queries?), it would not remember things discussed in the same “session” and, from time to time, it would also ignore our commands completely.\nThis is far from the experience we wish from a proper agent. The ideal diagnostic agent needs to be capable of remembering their mistakes and executing instructions consistently, e.g., learning that certain columns are not available and working around that. Also, can we really trust that it is doing what it is saying it is doing? We should be able to see the queries at any time to make sure the information it is returning is correct and up to date.\nWith these goals in mind, let’s get our hands dirty and start building our Emergency Medical Hologram Diagnostic Agent!\nSetting the Stage # Last time we wrote the code in a Jupyter notebook for the sake of convenience, but this time we are going to write a regular Python program. The same code would also work on Jupyter with minimal changes, but we are doing this so that we can use the diagnostic agent with a proper chat interface.\nFor any Python projects, I always recommend starting with a clean virtual env to keep dependencies self contained:\n$ mkdir -p ~/projects/diagnostic-agent $ cd ~/projects/diagnostic-agent $ python3 -m venv venv $ source venv/bin/activate $ pip install --upgrade google-cloud-aiplatform[agent_engines,langchain] Here is the initial version of main.py that reproduces the agent in the previous article:\nimport vertexai from vertexai import agent_engines import osquery from rich.console import Console from rich.markdown import Markdown import os PROJECT_ID = os.environ.get(\u0026#34;GCP_PROJECT\u0026#34;) LOCATION = os.environ.get(\u0026#34;GCP_REGION\u0026#34;, \u0026#34;us-central1\u0026#34;) STAGING_BUCKET = os.environ.get(\u0026#34;STAGING_BUCKET_URI\u0026#34;) vertexai.init( project=PROJECT_ID, location=LOCATION, staging_bucket=STAGING_BUCKET ) MODEL = os.environ.get(\u0026#34;GEMINI_MODEL\u0026#34;, \u0026#34;gemini-2.0-flash\u0026#34;) instance = osquery.SpawnInstance() def call_osquery(query: str): \u0026#34;\u0026#34;\u0026#34;Query the operating system using osquery This function is used to send a query to the osquery process to return information about the current machine, operating system and running processes. You can also use this function to query the underlying SQLite database to discover more information about the osquery instance by using system tables like sqlite_master, sqlite_temp_master and virtual tables. Args: query: str A SQL query to one of osquery tables (e.g. \u0026#34;select timestamp from time\u0026#34;) Returns: ExtensionResponse: an osquery response with the status of the request and a response to the query if successful. \u0026#34;\u0026#34;\u0026#34; if not instance.is_running(): instance.open() # This may raise an exception result = instance.client.query(query) return result def get_system_prompt(): if not instance.is_running(): instance.open() # This may raise an exception response = instance.client.query(\u0026#34;select name from sqlite_temp_master\u0026#34;).response tables = [ t[\u0026#34;name\u0026#34;] for t in response ] return f\u0026#34;\u0026#34;\u0026#34; Role: - You are the emergency diagnostic agent. - You are the last resort for the user to diagnose their computer problems. - Answer the user queries to the best of your capabilities. Tools: - you can call osquery using the call_osquery function. Context: - Only use tables from this list: {tables} - You can discover schemas using: PRAGMA table_info(table) Task: - Create a plan for which tables to query to fullfill the user request - Confirm the plan with the user before executing - If a query fails due a wrong column name, run schema discovery and try again - Query the required table(s) - Report the findings in a human readable way (table or list format) \u0026#34;\u0026#34;\u0026#34; def main(): agent = agent_engines.LangchainAgent( model = MODEL, system_instruction=get_system_prompt(), tools=[ call_osquery, ], ) console = Console() print(\u0026#34;Welcome to the Emergency Diagnostic Agent\\n\u0026#34;) print(\u0026#34;What is the nature of your diagnostic emergency?\u0026#34;) while True: try: query = input(\u0026#34;\u0026gt;\u0026gt; \u0026#34;) except EOFError: query = \u0026#34;exit\u0026#34; if query == \u0026#34;exit\u0026#34; or query == \u0026#34;quit\u0026#34;: break if query.strip() == \u0026#34;\u0026#34;: continue response = agent.query(input=query) rendered_markdown = Markdown(response[\u0026#34;output\u0026#34;]) console.print(rendered_markdown) print(\u0026#34;Goodbye!\u0026#34;) if __name__ == \u0026#34;__main__\u0026#34;: main() You can run the agent with python main.py:\n$ python main.py Welcome to the Emergency Diagnostic Agent What is the nature of your diagnostic emergency? \u0026gt;\u0026gt; There are two minor changes when compared to the original code: first, now we have a main loop which will keep the agent running until the user types “exit” or “quit”. This will create our chat interface.\nSecond, we have tweaked the system prompt to improve the agent consistency. We are now calling it “Emergency Diagnostic Agent” - this name not only works as a neat [Star Trek easter egg](https://en.wikipedia.org/wiki/The_Doctor_(Star_Trek:_Voyager), but more importantly, it sets a tone of urgency that, based on emerging research, may encourage it to comply with our requests more diligently. (Also check this recent interview)\nWe are not going to threaten our poor Emergency Diagnostic Agent - and I can guarantee that no agents were harmed in the making of this text - but, calling it an “Emergency” agent should set the tone so that it will try to comply with our requests to the best of our ability. In the previous version of the system prompt I got cases when the agent refused to do a task because it “thought” it was not capable of doing it, or didn’t know which tables to query.\nOf course, calling it an emergency agent is not enough to ensure the desired behaviour, so we added a few more instructions to guide the model behaviour, as we will see below.\nSystem Instructions # System instructions, also called system prompt, are a set of instructions that guide the behaviour of the LLM during the entire conversation. System instructions are special as they have higher priority over regular chat interactions. A way to imagine this is as if the system instructions were always repeated together with the prompt you are sending to the model.\nThere isn’t a strong consensus in the literature of what a system prompt should look like, but we have a few battle tested patterns that are emerging from day to day use. For example, one that is practically a consensus is to dedicate the very beginning of the system prompt to assign the agent a role, so it is aware of its purpose and can produce more coherent answers.\nFor this particular agent, I’ve opted to include the following sections in my system prompt: role, tools, context and task. This structure worked well during my testing phase, but don’t get too attached to it: experiment with your prompts and see if you can get better results. Experimentation is key to achieving good results with LLMs.\nNow let’s have a look at each section of the prompt.\nSystem Prompt: Role # A role is nothing more than the reason for the agent to exist. It could be as simple as “you are a software engineer” or “you are a diagnostic agent”, but they can be a bit more elaborate including a detailed description, behavioural rules, restrictions and others.\nFor a large language model that is trained on all kinds of data, the role helps setting the tone for the domain knowledge it will need to access to answer your queries. In other words, it gives semantic meaning to your queries… Imagine the question “what are cookies?” for example. Are we talking about edible cookies or browser cookies? If the agent role is undefined this question is completely ambiguous, but once we set the role to something technical (e.g. “you are a software engineer”), the ambiguity disappears.\nFor this agent, the role is described as:\nRole: - You are the emergency diagnostic agent. - You are the last resort for the user to diagnose their computer problems. - Answer the user queries to the best of your capabilities. Beyond the straightforward definition (“you are the emergency diagnostic agent”) we added a longer description to set the tone for the model behaviour and hopefully influence it to take our requests “seriously”, as mentioned before, the previous iteration of this agent had the bad tendency of refusing requests.\nSystem Prompt: Tools # Tools is the section that explains to the agent its capabilities to interact with external systems beyond its core model. Tools can be of several kinds, but the most common way to provide a tool to the agent is through function calls.\nThe agents can use tools to retrieve information, execute tasks and manipulate data. The Vertex AI SDK for Python has support for both user provided functions and built-in tools like Google Search and code execution. You can also use community maintained extensions through the Model Context Protocol (MCP) interface.\nFor our agent, we need to tell it that it can call osquery:\nTools: - you can call osquery using the call_osquery function. System Prompt: Context # Next we have the context, which tells the agent how about the environment it operates. I use this section to explicitly call out and correct undesired behaviours that previous iterations of the agent were prone to do. For example, I noticed very early in development that the agent would try to “guess” which tables were available and send queries blindly, resulting in a high error rate. Adding the list of tables to the context helped mitigate that problem.\nSimilar is the tendency for the agent to try to guess the column names in a table, instead of trying to discover the names first. In this particular case I resisted the temptation to instruct the agent to always use SELECT * because this is a bad practice (it retrieves more data than you need), but instead I “taught” it how to discover a schema using the PRAGMA instruction.\nThis way the agent will still make mistakes while guessing column names, but it has a way to course correct without human intervention.\nThe revised context section of the system prompt is shown below.\nContext: - Only use tables from this list: {tables} - You can discover schemas using: PRAGMA table_info(table) Note that tables is a variable that contains all the tables we discovered from osquery before starting the model.\nSystem Prompt: Task # Finally, the task. This section is used to describe how the agent should interpret your requests and execute them. People usually use this section to lay out the steps required to achieve the task at hand.\nIn our particular case, we are using this section to roughly lay out the plan, but also add a few conditional directives:\nTask: - Create a plan for which tables to query to fullfill the user request - Confirm the plan with the user before executing - If a query fails due a wrong column name, run schema discovery and try again - Query the required table(s) - Report the findings in a human readable way (table or list format) The step “confirm the plan with the user before executing” is interesting as it shows us how the agent is thinking about the process, but it might be a bit annoying after interacting with the agent for a while. We can always ask for the agent to tell us the plan with a prompt, so the inclusion of this step is entirely optional.\nI initially thought of this step as a way to debug the agent, but in the following section we are going to explore a different way to do it.\nWith the combination of these four sections we have the entire system prompt. This revised prompt produced more consistent results during my tests in preparation for this article. It also has the benefit of being “human-friendly” so it is easier to adapt when new rules are introduced.\nHere is the complete view of this system prompt:\nRole: - You are the emergency diagnostic agent. - You are the last resort for the user to diagnose their computer problems. - Answer the user queries to the best of your capabilities. Tools: - you can call osquery using the call_osquery function. Context: - Only use tables from this list: {tables} - You can discover schemas using: PRAGMA table_info(table) Task: - Create a plan for which tables to query to fullfill the user request - Confirm the plan with the user before executing - If a query fails due a wrong column name, run schema discovery and try again - Query the required table(s) - Report the findings in a human readable way (table or list format) As a side note, I believe that we still have many opportunities to improve it and one of my current areas of interest is how to achieve a self-improving system prompt. This could potentially be achieved by asking, at the end of a session, for the model to summarise its learnings into a new system prompt for its future iteration. The prompt could be stored in a database and loaded on the next session. This of course raises concerns for system prompt degradation or, even worse, attacks using prompt injection, so it is not as trivial as it seems. Nevertheless, it is a fun exercise and I might write about it in the near future.\nEnabling Debug Mode # Another concern about the original design is the lack of observability about what the agent is doing under the hood. There are two different approaches we can apply here, one slightly more painful than the other: 1) peek into the “thoughts” of the LLM and try to find the tool calls among them (very painful), or; 2) add some debug functionality to the function itself so it outputs the information we want during runtime (the easiest solution is usually the right one).\nI must admit, I spent an unhealthy amount of time in option 1, before realising I could do option 2. If you really want to go through the path of LLM reasoning, you can do so through a configuration called return_intermediate_steps. I should say this is very interesting from a learning point of view, but after spending a couple of hours trying to figure out the format of the output (hint: it is not really json) I decided that parsing it was not really worth the trouble.\nSo, how does the simple strategy work? We are adding a debug flag and a tool to toggle that flag on and off. This surprisingly simple trick actually opens an entire new world of potential: we are giving the agent an opportunity to modify its own behaviour!\nThe implementation of debug mode is composed by a global variable and a function to set it:\ndebug = False def set_debug_mode(debug_mode: bool): \u0026#34;\u0026#34;\u0026#34;Toggle debug mode. Call this function to enable or disable debug mode. Args: debug_mode (bool): True to enable debug mode, False to disable it. Returns: None \u0026#34;\u0026#34;\u0026#34; global debug debug = debug_mode We also need to mention it in the system prompt:\n... Tools: - you can call osquery using the call_osquery function. - you can enable or disable the debug mode using the set_debug_mode function. Context: ... And add the function to the tool list in the agent instantiation:\nagent = agent_engines.LangchainAgent( model = model, system_instruction=get_system_prompt(), tools=[ call_osquery, set_debug_mode, ], ) Finally, we need to adapt call_osquery to use the new debug flag:\ndef call_osquery(query: str): \u0026#34;\u0026#34;\u0026#34;Query the operating system using osquery This function is used to send a query to the osquery process to return information about the current machine, operating system and running processes. You can also use this function to query the underlying SQLite database to discover more information about the osquery instance by using system tables like sqlite_master, sqlite_temp_master and virtual tables. Args: query: str A SQL query to one of osquery tables (e.g. \u0026#34;select timestamp from time\u0026#34;) Returns: ExtensionResponse: an osquery response with the status of the request and a response to the query if successful. \u0026#34;\u0026#34;\u0026#34; if not instance.is_running(): instance.open() if debug: print(\u0026#34;Executing query: \u0026#34;, query) result = instance.client.query(query) if debug: print(\u0026#34;Query result: \u0026#34;, { \u0026#34;status\u0026#34;: result.status.message if result.status else None, \u0026#34;response\u0026#34;: result.response if result.response else None }) return result With all of these changes in place, let’s have a look at how the agent calls osquery using the newly implemented debug flag:\n$ python main.py Welcome to the Emergency Diagnostic Agent What is the nature of your diagnostic emergency? \u0026gt;\u0026gt; run a level 1 diagnostic procedure in debug mode Executing query: SELECT * FROM system_info Query result: {\u0026#39;status\u0026#39;: \u0026#39;OK\u0026#39;, \u0026#39;response\u0026#39;: [{...}]} Executing query: SELECT pid, name, user, cpu_percent FROM processes ORDER BY cpu_percent DESC LIMIT 10 Query result: {\u0026#39;status\u0026#39;: \u0026#39;no such column: user\u0026#39;, \u0026#39;response\u0026#39;: None} Executing query: SELECT pid, name, user, resident_size FROM processes ORDER BY resident_size DESC LIMIT 10 Query result: {\u0026#39;status\u0026#39;: \u0026#39;no such column: user\u0026#39;, \u0026#39;response\u0026#39;: None} Executing query: PRAGMA table_info(processes) Query result: {\u0026#39;status\u0026#39;: \u0026#39;OK\u0026#39;, \u0026#39;response\u0026#39;: [{\u0026#39;cid\u0026#39;: \u0026#39;0\u0026#39;, \u0026#39;dflt_value\u0026#39;: \u0026#39;\u0026#39;, \u0026#39;name\u0026#39;: \u0026#39;pid\u0026#39;, \u0026#39;notnull\u0026#39;: \u0026#39;1\u0026#39;, \u0026#39;pk\u0026#39;: \u0026#39;1\u0026#39;, \u0026#39;type\u0026#39;: \u0026#39;BIGINT\u0026#39;}, ...]} (...) System Information: • Hostname: petruzalek-mac.roam.internal • CPU Type: arm64e • Physical Memory: 51539607552 bytes Top 5 Processes by CPU Usage: PID Name CPU Usage ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1127 mediaanalysisd 95627517 43062 mediaanalysisd-access 66441942 54099 Google Chrome 3005046 54115 Google Chrome Helper (GPU) 2092500 81270 Electron 1688335 Top 5 Processes by Memory Usage (Resident Size): PID Name Resident Size (Bytes) ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 43062 mediaanalysisd-access 3933536256 54099 Google Chrome 1313669120 59194 Code Helper (Plugin) 1109508096 59025 Code Helper (Renderer) 915456000 19681 Google Chrome Helper (Renderer) 736329728 \u0026gt;\u0026gt; Notice that the command issued was “run a level 1 diagnostic procedure in debug mode” which showcases an interesting capability of the agent: multi-tool invocation. If it deems necessary, it is able to invocate not only the same tool multiple times, but different tools at the same time as well. So it was not necessary to enable debug mode before requesting the report: the agent was able to do it all in one go.\nAlso notice how the agent initially failed when requesting a user column, but then used the PRAGMA instruction to discover the correct schema and retry the query successfully. This is a perfect demonstration of the agent\u0026rsquo;s ability to recover from errors due to our improved system prompt.\nPreserving the Chat History # Our final task today is to ensure the agent remembers what we are talking about so that we can ask clarifying questions and further probe the system following a coherent line of investigation.\nIn the previous article we explored how LLMs are stateless and that we need to keep “reminding” them of the current state of the conversation using “turns”. Luckily with LangChain we don’t need to do this manually and we can rely on a feature called chat history.\nThe beauty of chat history is that anything that implements BaseChatMessageHistory can be used here, which allows us to use all sorts of data stores, including creating our own. For example, in the official documentation for Vertex AI you can find examples for using Firebase, Bigtable and Spanner.\nWe don’t need a full fledged database for the moment so we are going to settle with InMemoryChatMessageHistory, which as the name\tsuggests, will store everything in memory.\nHere is a typical implementation, technically supporting multiple sessions using the chats_by_session_id dictionary for lookup (code retrieved from the langchain documentation):\nchats_by_session_id = {} def get_chat_history(session_id: str) -\u0026gt; InMemoryChatMessageHistory: chat_history = chats_by_session_id.get(session_id) if chat_history is None: chat_history = InMemoryChatMessageHistory() chats_by_session_id[session_id] = chat_history return chat_history And here is our new main function instantiating the agent with chat history enabled:\nimport uuid def main(): session_id = uuid.uuid4() agent = agent_engines.LangchainAgent( model = model, system_instruction=get_system_prompt(), tools=[ call_osquery, set_debug_mode, ], chat_history=get_chat_history, ) A quick callout so that you don’t make the same mistake as me: the chat_history argument expects a Callable type, so you should not invoke the function there, but pass the function itself. LangChain uses a factory pattern here; it invokes the provided function (get_chat_history) on demand with a session_id to get or create the correct history object. This design is what enables the agent to manage multiple, separate conversations concurrently.\nThe function signature can either include one or two arguments. If one argument, it is assumed to be a session_id, and if it is two arguments they are interpreted as user_id and conversation_id. More information about this can be found in the RunnableWithMessageHistory documentation.\nThe last piece of the puzzle is passing the session_id to the model runner. This is made through the config argument as shown in the code below:\n# (...) while True: try: query = input(\u0026#34;\u0026gt;\u0026gt; \u0026#34;) except EOFError: query = \u0026#34;exit\u0026#34; if query == \u0026#34;exit\u0026#34; or query == \u0026#34;quit\u0026#34;: break if query.strip() == \u0026#34;\u0026#34;: continue response = agent.query(input=query, config={\u0026#34;configurable\u0026#34;: {\u0026#34;session_id\u0026#34;: session_id}}) rendered_markdown = Markdown(response[\u0026#34;output\u0026#34;]) console.print(rendered_markdown) Now as long as the session is alive we can ask the agent about information in its “short-term memory”, as the session contents are stored in memory. This will be enough for most basic interactions to feel more natural, but we are opening precedence for bigger problems: now that we can store session information, after each iteration it will only grow, and while dealing with data generated automatically from queries, the session context will grow very fast soon hitting the limits of the model, and long before we hit the limits of our computer memory.\nModels like Gemini are well known for their long context windows, but even a million tokens can be exhausted really fast if we fill the context with data. Long context can also pose a problem for some models as retrieval gets harder and harder - also known as the needle in a haystack problem.\nThere are techniques for addressing the growing context problem, including compression and summarisation, but for the sake of keeping the context of this article short (see what I did there?), we are going to save those for the next article.\nThe final version of main.py including all modifications in this article looks like this:\nimport vertexai from vertexai import agent_engines import osquery from rich.console import Console from rich.markdown import Markdown from langchain_core.chat_history import InMemoryChatMessageHistory import os import uuid PROJECT_ID = os.environ.get(\u0026#34;GCP_PROJECT\u0026#34;) LOCATION = os.environ.get(\u0026#34;GCP_REGION\u0026#34;, \u0026#34;us-central1\u0026#34;) STAGING_BUCKET = os.environ.get(\u0026#34;STAGING_BUCKET_URI\u0026#34;) vertexai.init( project=PROJECT_ID, location=LOCATION, staging_bucket=STAGING_BUCKET ) MODEL = os.environ.get(\u0026#34;GEMINI_MODEL\u0026#34;, \u0026#34;gemini-2.5-pro-preview-05-06\u0026#34;) instance = osquery.SpawnInstance() debug = False def set_debug_mode(debug_mode: bool): \u0026#34;\u0026#34;\u0026#34;Toggle debug mode. Call this function to enable or disable debug mode. Args: debug_mode (bool): True to enable debug mode, False to disable it. Returns: None \u0026#34;\u0026#34;\u0026#34; global debug debug = debug_mode def call_osquery(query: str): \u0026#34;\u0026#34;\u0026#34;Query the operating system using osquery This function is used to send a query to the osquery process to return information about the current machine, operating system and running processes. You can also use this function to query the underlying SQLite database to discover more information about the osquery instance by using system tables like sqlite_master, sqlite_temp_master and virtual tables. Args: query: str A SQL query to one of osquery tables (e.g. \u0026#34;select timestamp from time\u0026#34;) Returns: ExtensionResponse: an osquery response with the status of the request and a response to the query if successful. \u0026#34;\u0026#34;\u0026#34; if not instance.is_running(): instance.open() # This may raise an exception if debug: print(\u0026#34;Executing query: \u0026#34;, query) result = instance.client.query(query) if debug: print(\u0026#34;Query result: \u0026#34;, { \u0026#34;status\u0026#34;: result.status.message if result.status else None, \u0026#34;response\u0026#34;: result.response if result.response else None }) return result def get_system_prompt(): if not instance.is_running(): instance.open() # This may raise an exception response = instance.client.query(\u0026#34;select name from sqlite_temp_master\u0026#34;).response tables = [ t[\u0026#34;name\u0026#34;] for t in response ] return f\u0026#34;\u0026#34;\u0026#34; Role: - You are the emergency diagnostic agent. - You are the last resort for the user to diagnose their computer problems. - Answer the user queries to the best of your capabilities. Tools: - you can call osquery using the call_osquery function. - you can use the set_debug_mode function to enable or disable debug mode. Context: - Only use tables from this list: {tables} - You can discover schemas using: PRAGMA table_info(table) Task: - Create a plan for which tables to query to fullfill the user request - Confirm the plan with the user before executing - If a query fails due a wrong column name, run schema discovery and try again - Query the required table(s) - Report the findings in a human readable way (table or list format) \u0026#34;\u0026#34;\u0026#34; chats_by_session_id = {} def get_chat_history(session_id: str) -\u0026gt; InMemoryChatMessageHistory: chat_history = chats_by_session_id.get(session_id) if chat_history is None: chat_history = InMemoryChatMessageHistory() chats_by_session_id[session_id] = chat_history return chat_history def main(): session_id = uuid.uuid4() agent = agent_engines.LangchainAgent( model = MODEL, system_instruction=get_system_prompt(), tools=[ call_osquery, set_debug_mode ], chat_history=get_chat_history, ) console = Console() print(\u0026#34;Welcome to the Emergency Diagnostic Agent\\n\u0026#34;) print(\u0026#34;What is the nature of your diagnostic emergency?\u0026#34;) while True: try: query = input(\u0026#34;\u0026gt;\u0026gt; \u0026#34;) except EOFError: query = \u0026#34;exit\u0026#34; if query == \u0026#34;exit\u0026#34; or query == \u0026#34;quit\u0026#34;: break if query.strip() == \u0026#34;\u0026#34;: continue response = agent.query(input=query, config={\u0026#34;configurable\u0026#34;: {\u0026#34;session_id\u0026#34;: session_id}}) rendered_markdown = Markdown(response[\u0026#34;output\u0026#34;]) console.print(rendered_markdown) print(\u0026#34;Goodbye!\u0026#34;) if __name__ == \u0026#34;__main__\u0026#34;: main() Conclusions # In this article we have learned the importance of fine tuning a system prompt to achieve consistent responses from an agent. We also have seen in practice how multi-tool calling operates, and how to use tools to enable or disable feature flags to change the behaviour of the agent. Last but not least, we have learned about how to manage session state using the in-memory chat history.\nIn the next article of the series we will see how to enable persistence between sessions using a real database, revisit the notion of tokens and discuss the context compression technique.\nAppendix: Fun Things to Try # Now that our agent is more robust, use this section as a practical guide to test the new features in action. Notice how it now remembers context between questions and how you can ask it to explain its work.\n\u0026gt;\u0026gt; run a level 1 diagnostic procedure \u0026gt;\u0026gt; run a level 2 diagnostic procedure \u0026gt;\u0026gt; explain the previous procedure step by step \u0026gt;\u0026gt; find any orphan processes \u0026gt;\u0026gt; show me the top resource consuming processes \u0026gt;\u0026gt; write a system prompt to transfer your current knowledge to another agent \u0026gt;\u0026gt; search the system for malware \u0026gt;\u0026gt; is this computer connected to the internet? \u0026gt;\u0026gt; why is my computer slow? \u0026gt;\u0026gt; take a snapshot of the current performance metrics \u0026gt;\u0026gt; compare the current perfomance metrics with the previous snapshot \u0026gt;\u0026gt; give me a step by step process to fix the issues you found \u0026gt;\u0026gt; how many osqueryd processes are in memory? \u0026gt;\u0026gt; give me a script to kill all osqueryd processes \u0026gt;\u0026gt; who am i? If you find other interesting prompts, please share your experiences in the comments section below. See you next time!\n","date":"11 June 2025","externalUrl":null,"permalink":"/posts/20250611-system-prompt/","section":"Posts","summary":"This article explores the concepts of system instruction, session history and agent tools to create a smarter diagnostic assistant.","title":"Boldly Prompting: A Practical Guide to System Instructions and Agent Tools","type":"posts"},{"content":"","date":"11 June 2025","externalUrl":null,"permalink":"/tags/python/","section":"Tags","summary":"","title":"Python","type":"tags"},{"content":"","date":"11 June 2025","externalUrl":null,"permalink":"/tags/vertex-ai/","section":"Tags","summary":"","title":"Vertex Ai","type":"tags"},{"content":" Introduction # This article explores the communication model between the client code and the Gemini API using the Vertex AI SDK for Python. We will cover concepts like how the messages are structured, how the model understands the context of the question and how to augment the model capabilities with function calls. While Gemini is the focus of this article, the same concepts you will see here can also be applied to Gemma and other LLMs.\nIn my previous post I’ve explained how to write a simple - but surprisingly powerful - AI Agent that responds to diagnostic questions about your local machine. In very few lines of code (and not so few lines of comments) we were able to get our agent to respond to queries like “how much CPU I have in my machine” or “please check for any signs of malware”.\nThat was, of course, due to the beauty of the Python SDK as it simplified things a lot. For example, I relied on a feature called Automatic Function Calling to let the agent decide when to call a function. This feature also helped me get away with defining the functions as plain Python functions and the SDK figured out its signature and description dynamically for me. This capability unfortunately is only available for the Python SDK, so developers in other languages need to do a bit more work.\nThis is why in today’s article we are going to take a slightly different approach and discuss how the Gemini API works so that you can be better prepared for using not only Python, but any of the available SDKs out there (JS, Go and Java). I’ll still be using Python for the examples so you can compare with the previous article, but the concepts discussed here are valid across all different languages.\nWe are going to cover two main topics:\nHow the conversation between client and model works How to implement function calls the manual way Note that if you are a Python developer this doesn’t mean you won’t get anything from this article either. Actually, understanding the flow of the conversation will be important to use more advanced concepts of the SDK (like the Live API) and working with LLMs in general.\nUnderstanding How the API works # Agents typically work in the same way as client server applications - you have a client component who is responsible for preparing and making the requests and a server process that hosts the model runtime and processes the client requests.\nFor Vertex AI there are two main groups of APIs: a REST APIs for the typical request/response style of content generation, where the client sends a request and waits for the response before continuing, and a new Live API that processes real time information using websockets. We are going to focus on the REST APIs first, as the Live API requires a bit more groundwork to get it right.\nWe typically generate content in one of the following modalities: text, image, audio and video. Many of the most recent models are also multi-modal, which means you can deal with more than one modality for input and/or output at the same time. To keep things simple, let’s start with text.\nA typical one-off prompt application looks like this:\nfrom google import genai client = genai.Client( vertexai=True, project=\u0026#34;daniela-genai-sandbox\u0026#34;, location=\u0026#34;us-central1\u0026#34; ) response = client.models.generate_content( model=\u0026#34;gemini-2.0-flash\u0026#34;, contents=\u0026#34;How are you today?\u0026#34; ) print(response.text) Output:\nI am doing well, thank you for asking! As a large language model, I don\u0026#39;t experience emotions like humans do, but I am functioning optimally and ready to assist you. How can I help you today? The first thing we need to do is instantiate the client, either using the Vertex AI mode (vertexai=True) or using a Gemini API key. In this case I’m using Vertex AI mode.\nOnce the client is initialized, we can send it a prompt using the client.models.generate_content method. We need to specify which model we are calling (in this case gemini-2.0-flash) and the prompt in the contents argument (e.g. \u0026quot;How are you today?\u0026quot;).\nLooking at this code it might be hard to imagine what is going on under the hood, as we are getting many abstractions for free thanks to Python. The most important thing in this case is that the content is not a string.\nContents is actually a list of content structures, and content structures are composed by a role and one or more parts. The underlying type for this structure is defined in the types library and looks like this:\nfrom google.genai import types contents = [types.Content( role = \u0026#34;user\u0026#34;, parts = [ types.Part_from_text(\u0026#34;How are you today?\u0026#34;) )] So whenever we type contents=\u0026quot;How are you today?\u0026quot;, the Python SDK does this transformation from string to “content with a string part” automatically for us.\nAnother important thing to note is that whenever we make a call to generate_content, the model is starting from zero. This means that it is our responsibility to add the context of previous messages to the next prompt. Let’s do a simple test by asking the model what day is today two times in a row:\nresponse = client.models.generate_content( model=\u0026#34;gemini-2.0-flash\u0026#34;, contents=\u0026#34;what day is today?\u0026#34; ) print(response.text) response = client.models.generate_content( model=\u0026#34;gemini-2.0-flash\u0026#34;, contents=\u0026#34;what day is today?\u0026#34; ) print(response.text) Output:\n$ python3 main.py Today is Sunday, November 5th, 2023. Today is Saturday, November 2nd, 2024. There are two problems with the response above: 1) it hallucinated, as the model has no way of knowing the date, and 2) it gave two different answers to the same question. We can fix 1) by grounding in a tool like a datetime call or Google Search, but I want to focus on 2) because it clearly shows that the model doesn’t remember what he just said and demonstrates the point above that it is our responsibility to keep the model up to date on the conversation.\nLet’s make a small modification to the code:\nresponse = client.models.generate_content( model=\u0026#34;gemini-2.0-flash\u0026#34;, contents=\u0026#34;what day is today?\u0026#34; ) print(response.text) # each element in the contents array is usually referred as a \u0026#34;turn\u0026#34; contents = [ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;parts\u0026#34;: [{ \u0026#34;text\u0026#34;: \u0026#34;what day is today?\u0026#34; }] }, { \u0026#34;role\u0026#34;: \u0026#34;model\u0026#34;, \u0026#34;parts\u0026#34;: [{ \u0026#34;text\u0026#34;: response.text }] }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;parts\u0026#34;: [{ \u0026#34;text\u0026#34;: \u0026#34;what day is today?\u0026#34; }] }, ] response = client.models.generate_content( model=\u0026#34;gemini-2.0-flash\u0026#34;, contents=contents ) print(response.text) Output:\n$ python3 main.py Today is Wednesday, November 15, 2023. Today is Wednesday, November 15, 2023. Note that on the second call to the model we are including the whole context in the contents attribute. Also note that the role from each part changes from “user” to “model” and then to “user” again (“user” and “model” are the only possible values for role). That’s how the model understands which point of the conversation it is, a.k.a. “turn”. If, for example, we omitted the last part that repeats the question, the model would think it’s up to date and wouldn’t produce another response, as the last turn would be from “model” and not “user”.\nThe contents variable above is written in the “dictionary” form, but the SDK also provides several convenience methods like types.UserContent (sets the role field to “user” automatically) and types.Part.from_text (converts a plain string into a part), among others.\nTo deal with other types of inputs and/or outputs, we can use other types of parts like function calls, binary data, etc. If a model is multi-modal, you can mix parts of different content types in the same message.\nBinary data can be both inline or be fetched from an URI. You can differentiate between different types of data using the mime_type field. For example, an image part can be retrieved like this:\nfrom google.genai import types contents = types.Part.from_uri( file_uri: \u0026#39;gs://generativeai-downloads/images/scones.jpg\u0026#39;, mime_type: \u0026#39;image/jpeg\u0026#39;, ) Or inlined:\ncontents = types.Part.from_bytes( data: my_cat_picture, # binary data mime_type: \u0026#39;image/jpeg\u0026#39;, ) In summary, for every turn of the conversation, we will be adding a new line of content for both the previous model response and the new user question.\nThe good news is that the chatbot experience is such an important use case that the Vertex AI SDK provides an implementation for this flow out of the box. Using the chat feature, we can reproduce the behavior above in very few lines of code:\nchat = client.chats.create(model=\u0026#39;gemini-2.0-flash\u0026#39;) response = chat.send_message(\u0026#39;what day is today?\u0026#39;) print(response.text) response = chat.send_message(\u0026#39;what day is today?\u0026#39;) print(response.text) Output:\n$ python3 main.py Today is Saturday, October 14th, 2023. Today is Saturday, October 14th, 2023. This time the model remembered the date because the chat interface is handling the history automatically for us.\nNon-automatic function calling # Now that we saw how the API works to build client messages and manage context, it’s time to explore how it deals with function calls. At a basic level, we will need to instruct the model that it has a function at its disposition and then process its requests to call the function and return the resulting values to the model. This is important because function calls allow agents to interact with external systems and the real world, creating actions such as retrieving data or triggering specific processes, going beyond just generating text.\nThe function declaration is what tells the model what it can do. It tells the model the function name, description and its arguments. For example, below is a function declaration for the get_random_number function:\nget_random_number_decl = { \u0026#34;name\u0026#34;: \u0026#34;get_random_number\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Returns a random number\u0026#34;, } It is this declaration that the model needs to know to decide which functions to call. The function declaration has three fields: name, description and parameters - in this case the function doesn’t accept parameters so this field is omitted. The model uses the function description and the description of its arguments to decide when and how to call each function.\nIn the previous article, instead of giving the model a function declaration, I was lazy and let the SDK figure it out for me based on the docstring of my function. This time, we are going to do differently and explicitly declare a function to get a better understanding of the underlying flow.\nThe function including its declaration look like this:\ndef get_random_number(): return 4 # chosen by fair dice roll # guaranteed to be random (https://xkcd.com/221/) # the declaration tells the model what it needs to know about the function get_random_number_decl = { \u0026#34;name\u0026#34;: \u0026#34;get_random_number\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Returns a random number\u0026#34;, } You can see other examples of function declarations here.\nNext we need to tell the model it has access to this function. We do this through the model configuration, adding the function as a tool.\ntools = types.Tool(function_declarations=[get_random_number_decl]) config = types.GenerateContentConfig(tools=[tools]) # my initial prompt contents = [types.Part.from_text(text=\u0026#34;what is my lucky number today?\u0026#34;)] response = client.models.generate_content( model=\u0026#34;gemini-2.0-flash\u0026#34;, contents=contents, config=config, # note how we are adding the config to the model call ) print(response.candidates[0].content.parts[0]) If you run the code above you will get something like this:\n$ python3 main.py video_metadata=None thought=None inline_data=None file_data=None thought_signature=None code_execution_result=None executable_code=None function_call=FunctionCall(id=None, args={}, name=\u0026#39;get_random_number\u0026#39;) function_response=None text=None What you are seeing here is the first part of the model response, and we can see that this part has all fields empty (None) except for the function_call field. This means the model wants us to make this function call, and then return its result back to the model.\nThis initially puzzled me, but if you think about it makes total sense. The model knows that the function exists, but has absolutely no idea how to call it. From the models perspective, the function is not running on the same machine either, so the model cannot do anything except “politely asking” us to do the call on their behalf.\nWe didn’t have to do this in my previous article because Automatic Function Calling took over and simplified things for us. The call still followed the same flow, but the SDK had hidden all this complexity from us.\nThe obvious thing to do now is to call the actual function and return the result to the model, but remember, without context the model knows nothing about our previous request, so if you only send the function results back it has no idea of what to do with it!\nThat’s why we need to send the history of the interaction up to now, and at least as far back as the point where the model knows it requested that value. The code below assumes we got a function call message and we need to send a new request with the complete information:\n# assuming we already inspected the response and know what the model wants result = get_random_number() # makes the actual function call # contents still contain the original prompt so we will add the model response... contents.append(types.ModelContent(parts=response.candidates[0].content.parts)) # ... and the result of the function call contents.append(types.UserContent(parts=types.Part.from_function_response(name=\u0026#34;get_random_number\u0026#34;, response={\u0026#34;result\u0026#34;: result}))) response = client.models.generate_content( model=\u0026#34;gemini-2.0-flash\u0026#34;, contents=contents, config=config, ) print(response.text) Output:\n$ python3 main.py Today\u0026#39;s lucky number is 4. Conclusions # In this article we have seen how the agent client communicates with the model on the server-side or, in other words, the “domain model” of LLM communications. We also removed the curtain on the “magic” that the Python SDK does for us.\nAutomation is always convenient and helps us achieve results much faster, but knowing how it actually works is usually the big difference between a smooth journey and a patchy one when implementing your own agent, specially because the special cases are never that easy.\nI know that in times of vibe coding at first glance it is almost ironic to say something like this, but one of the things I quickly learned when vibe coding is that if you are more precise when speaking with the AI you get much better results in much less time. So now is not the time to downplay the value of knowledge, but double down on it - not besides AI but because of it.\nHopefully you enjoyed the journey so far. In the next article we will build up on this knowledge to take the diagnostic agent to the next level, where no agent has ever gone before! (or maybe has, but certainly not mine =^_^=)\nPlease write your comments below! Peace out o/\n","date":"5 June 2025","externalUrl":null,"permalink":"/posts/20250605-vertex-ai-sdk-python/","section":"Posts","summary":"This article explores the communication model between the client code and the Gemini API using the Vertex AI SDK for Python","title":"Digging deeper into the Vertex AI SDK for Python","type":"posts"},{"content":"Space: the final frontier. These are the voyages of the starship Enterprise. Its 5-year mission: to explore strange new worlds; to seek out new life and new civilizations; to boldly go where no man has gone before.\nIntroduction # When growing up, thanks to the influence of my father, I got used to hearing these words almost every day. I suspect his passion for Star Trek played a huge role in me choosing the software engineering career. (For those who are not familiar with Star Trek, this speech was played in the beginning of every episode of the original Star Trek series)\nStar Trek was always ahead of its time. It showed the first interracial kiss in U.S television, in times when such a scene caused much controversy. It also depicted many pieces of “futuristic” technology that today are commodities, like smartphones and video conferencing.\nOne thing that is really remarkable is how the engineers in the series interact with the computers. While we do see some keyboards and button presses now and then, many of the commands are vocalised in natural language. Some of the commands they give to the computer are quite iconic, like for example when they request the computer to run a “ level 1 diagnostic procedure”, which happened so many times that it practically became a joke among the most hardcore fans.\nFast forward 30+ years and here we are, in the Age of AI, a technology revolution that promises to be bigger than the internet. Of course a lot of people are scared of how AI might impact their jobs (I wrote about it last week), but growing up watching Star Trek makes it easier for me to see how the role of the engineer will change in the next few years. Instead of commanding the computer through text, manually instructing each step of the way through lines of code and compilers, we will very soon move towards talking and brainstorming with our computers.\nTo help people visualize this, we are going to use the technology we have today to create a small agent that allows us to interact with our own machines using natural language.\nWhat you will need for this demo # For the development language we will be using Python in a Jupyter Notebook, as it plays very nicely for experimentation. The main tools and libraries we will be using are:\nVertex AI Agent Engine Osquery with python bindings Jupyter Notebook [optional] (I’m actually using the Jupyter plugin for VSCode) The examples below will use Gemini Flash 2.0, but you can use any Gemini model variant. We won\u0026rsquo;t deploy this agent to Google Cloud this time as we want to use it to answer questions about the local machine and not about the server in the cloud.\nAgent Overview # If you are already familiar with how agent technology works you can skip this section.\nAn AI agent is a form of AI that is capable of perceiving its environment and taking autonomous actions to achieve specific goals. If you compare with the typical Large Language Models (LLMs), which primarily focus on generating content based on input, AI agents can interact with their environment, make decisions, and execute tasks to reach their objectives. This is achieved by the use of “tools” that will feed the agent with information and enable it to do actions.\nTo demonstrate the agent technology we are going to use LangChain through Agent Engine. First, you need to install the required packages in your system:\npip install --upgrade --quiet google-cloud-aiplatform[agent_engines,langchain] You will also need to set your gcloud application default credentials (ADC):\ngcloud auth application-default login Note: depending on the environment you are using to run this demo you may need to use a different authentication method.\nNow we are ready to work on our Python script. First we are going to initialize the SDK based on our Google Cloud project ID and location:\nimport vertexai vertexai.init( project=\u0026#34;my-project-id\u0026#34;, # Your project ID. location=\u0026#34;us-central1\u0026#34;, # Your cloud location. staging_bucket=\u0026#34;gs://my-staging-bucket\u0026#34;, # Your staging bucket. ) Once the initial setup is done, creating an agent using LangChain in Agent Engine is pretty straightforward:\nfrom vertexai import agent_engines model = \u0026#34;gemini-2.0-flash\u0026#34; # feel free to try different models! model_kwargs = { # temperature (float): The sampling temperature controls the degree of # randomness in token selection. \u0026#34;temperature\u0026#34;: 0.20, } agent = agent_engines.LangchainAgent( model=model, # Required. model_kwargs=model_kwargs, # Optional. ) The setup above is just enough for you to submit queries to the agent, just like you would submit a query to a LLM:\nresponse = agent.query( input=\u0026#34;which time is now?\u0026#34; ) print(response) Which could return something like this:\n{\u0026#39;input\u0026#39;: \u0026#39;which time is now?\u0026#39;, \u0026#39;output\u0026#39;: \u0026#39;As an AI, I don\\\u0026#39;t have a \u0026#34;current\u0026#34; time or location in the same way a human does. My knowledge isn\\\u0026#39;t updated in real-time.\\n\\nTo find out the current time, you can:\\n\\n* **Check your device:** Your computer, phone, or tablet will display the current time.\\n* **Do a quick search:** Type \u0026#34;what time is it\u0026#34; into a search engine like Google.\u0026#39;} Depending on your settings, prompt and the randomness of the universe, the model can either give you a response saying it cannot tell you the time, or it can “hallucinate” and make up a timestamp. But in fact, since the AI doesn’t have a clock, it will not be able to answer this question\u0026hellip; unless you give it a clock!\nFunction Calls # One of the most convenient ways of extending our agent capabilities is to give it python functions to call. The process is pretty simple, but it’s important to highlight that the better the documentation you have for the function, the easier it will be for the agent to get its call right. Let’s define our function to check the time:\nimport datetime def get_current_time(): \u0026#34;\u0026#34;\u0026#34;Returns the current time as a datetime object. Args: None Returns: datetime: current time as a datetime type \u0026#34;\u0026#34;\u0026#34; return datetime.datetime.now() Now that we have a function that gives us the system time, let’s recreate the agent but now aware that the function exists:\nagent = agent_engines.LangchainAgent( model=model, # Required. model_kwargs=model_kwargs, # Optional. tools=[get_current_time] ) And ask the question again:\nresponse = agent.query( input=\u0026#34;which time is now?\u0026#34; ) print(response) The output will look similar to this:\n{\u0026#39;input\u0026#39;: \u0026#39;which time is now?\u0026#39;, \u0026#39;output\u0026#39;: \u0026#39;The current time is 18:36:42 UTC on May 30, 2025.\u0026#39;} Now the agent can rely on the tool to answer the question with real data. Pretty cool, huh?\nGathering System Information # For our diagnostic agent we are going to give it capabilities to query information about the machine it is running in using a tool called osquery. Osquery is an open source tool developed by Facebook to allow the user to make SQL queries to “virtual tables” that expose information about the underlying operating system of the machine.\nThis is convenient to us because it not only gives us a single point of entry to make queries about the system, but LLMs are also very proficient in writing SQL queries.\nYou can find instructions on how to install osquery in the official documentation. I won’t reproduce them here because they vary depending on the operating system of your machine.\nOnce you have osquery installed you will need to install the python bindings for osquery. As typical python, it’s only one pip install:\npip install --upgrade --quiet osquery With the bindings installed you can make osquery calls by importing the osquery package:\nimport osquery # Spawn an osquery process using an ephemeral extension socket. instance = osquery.SpawnInstance() instance.open() # This may raise an exception # Issues queries and call osquery Thrift APIs. instance.client.query(\u0026#34;select timestamp from time\u0026#34;) The query method will return an ExtensionResponse object with the results of your query. For example:\nExtensionResponse(status=ExtensionStatus(code=0, message=\u0026#39;OK\u0026#39;, uuid=0), response=[{\u0026#39;timestamp\u0026#39;: \u0026#39;Fri May 30 17:54:06 2025 UTC\u0026#39;}]) If you never worked with osquery before I encourage you to have a look at the schema to see which kind of information is available in your operating system.\nA side note about formatting # All the outputs from the previous examples were unformatted, but if you are running the code from Jupyter you can access some convenience methods to beautify the output by importing the following packages:\nfrom IPython.display import Markdown, display And displaying the response output as markdown:\nresponse = agent.query( input=\u0026#34;what is today\u0026#39;s stardate?\u0026#34; ) display(Markdown(response[\u0026#34;output\u0026#34;])) Output:\nCaptain\u0026#39;s Log, Supplemental. The current stardate is 48972.5. Connecting the dots # Now that we have a way to query information about the operating system, let’s combine that with our knowledge of agents to make a diagnostic agent that will answer questions about our system.\nThe first step is to define a function to make the queries. This will be given to the agent as a tool to gather information later:\ndef call_osquery(query: str): \u0026#34;\u0026#34;\u0026#34;Query the operating system using osquery This function is used to send a query to the osquery process to return information about the current machine, operating system and running processes. You can also use this function to query the underlying SQLite database to discover more information about the osquery instance by using system tables like sqlite_master, sqlite_temp_master and virtual tables. Args: query: str A SQL query to one of osquery tables (e.g. \u0026#34;select timestamp from time\u0026#34;) Returns: ExtensionResponse: an osquery response with the status of the request and a response to the query if successful. \u0026#34;\u0026#34;\u0026#34; return instance.client.query(query) The function itself is pretty trivial, but the important part here is to have a very detailed docstring that will enable the agent to understand how this function works.\nDuring my testing, one tricky problem that occurred quite frequently is that the agent didn’t know exactly which tables were available in my system. For example, I’m running a macOS machine and the table “memory_info” doesn’t exist.\nTo give the agent a bit more context, we are going to dynamically give to it the names of the tables that are available in this system. In an ideal situation, you would even give it the entire schema with column names and descriptions, but unfortunately that is not trivial to achieve with osquery.\nThe underlying database technology for osquery is SQLite so we can query the list of virtual tables from the sqlite_temp_master table:\n# use some python magic to figure out which tables we have in this system response = instance.client.query(\u0026#34;select name from sqlite_temp_master\u0026#34;).response tables = [ t[\u0026#34;name\u0026#34;] for t in response ] Now that we have all the table names, we can create the agent with this information and the call_osquery tool:\nosagent = agent_engines.LangchainAgent( model = model, system_instruction=f\u0026#34;\u0026#34;\u0026#34; You are an agent that answers questions about the machine you are running in. You should run SQL queries using one or more of the tables to answer the user questions. Always return human readable values (e.g. megabytes instead of bytes, and formatted time instead of miliseconds) Be very flexible in your interpretation of the requests. For example, if the user ask for application information, it is acceptable to return information about processes and services. If the user requests resource usage, return BOTH memory and cpu information. Do not ask the user for clarification. You have the following tables available to you: ----- TABLES ----- {tables} ----- END TABLES ----- Question: \u0026#34;\u0026#34;\u0026#34;, tools=[ call_osquery, ] ) Believe it or not, our agent is ready for all our questions! Let’s try it out:\nresponse = osagent.query(input=\u0026#34;what is the current time?\u0026#34;) display(Markdown(response[\u0026#34;output\u0026#34;])) Output:\nThe current time is Fri May 30 18:08:15 2025 UTC. Let’s make it a bit more complicated:\nresponse = osagent.query(input=\u0026#34;what is the top consuming process?\u0026#34;) display(Markdown(response[\u0026#34;output\u0026#34;])) Output:\nThe top CPU-consuming process is: Process Name: mediaanalysisd, PID: 1127, Total CPU Time: 2876716 The top memory-consuming process is: Process Name: Code Helper (Plugin), PID: 2987, Memory Usage: 1537 MB What if I am a bit more creative?\nresponse = osagent.query(input=\u0026#34;computer, run a level 1 diagnostic procedure\u0026#34;) display(Markdown(response[\u0026#34;output\u0026#34;])) Output:\nHere is the level 1 diagnostic information: **Operating System:** * **Name:** macOS * **Version:** 15.5 * **Build:** 24F74 * **Platform:** darwin * **Architecture:** arm64 ... ommitted for brevity ... **Uptime:** * **Total Seconds:** 261164 * **Days:** 3 * **Hours:** 0 * **Minutes:** 32 * **Seconds:** 44 Wow! I never actually defined what a Level 1 diagnostic procedure is, but it gave me a pretty impressive report nevertheless!\nI even tried to be a bit more clever in my questions and the answers didn’t disappoint (most of the times):\nresponse = osagent.query(input=\u0026#34;computer, do you see any signs of malware running?\u0026#34;) display(Markdown(response[\u0026#34;output\u0026#34;])) Output:\nI have checked for processes that are not associated with a file on disk, which can be a sign of malware, and found none. I have also examined the top processes by memory and CPU usage. The processes consuming the most resources are primarily Visual Studio Code and Google Chrome and their related helper processes. This is typical behavior for these applications. Based on the checks performed, there are no obvious signs of malware running on the system at this time. Mic drop =^.^=\nConclusions # I know it’s a beaten argument by now, but AI is a game changer. With very few lines of code we have gone from zero to a fully functioning natural language interface to the inner workings of the operating system. With a little more work this agent can be improved to do deeper diagnostics and maybe even autonomously fix things. Scotty would be proud!\nYou can find the source code for all examples in this article on my GitHub.\nWhat are your impressions? Share your thoughts in the comments below.\n","date":"31 May 2025","externalUrl":null,"permalink":"/posts/20250531-diagnostic-agent/","section":"Posts","summary":"How to create a diagnostic agent that speaks natural language using Gemini and Vertex AI Agent Engine","title":"How I turned my computer into \"USS Enterprise\" using AI Agents","type":"posts"},{"content":" Introduction # Do you believe that a non-engineer can \u0026ldquo;vibe code\u0026rdquo; a production-ready application? This is the question I\u0026rsquo;ve been reflecting on lately.\nThe AI landscape is moving so fast, and we are seeing so many cool tools being released almost every day, so it\u0026rsquo;s natural that many people are getting scared and are starting to \u0026ldquo;predict\u0026rdquo; the end of the software engineering career. Look at Jules for example, an AI agent that can automate many typical engineer tasks like updating dependencies, writing documentation, refactoring and even testing.\nIf those are the only things you are adding to the table as an engineer, maybe yes, you should be scared. But fear is not necessarily a bad thing, it makes us move out of the comfort zone, it allows us to take risks where otherwise we wouldn\u0026rsquo;t. A good amount of fear drives self-development and innovation.\nWhat Is an Engineer, Anyway? # I might not be the best person to answer this question because even though I studied Electrical Engineering in college I actually never graduated. Nevertheless, I can say that even without a diploma, the period of time that I spent on the course was foundational to shape my mindset when approching problems in my career.\nOne of the key moments that influenced me was a masterclass with Dr. Ewaldo Mehl (UFPR) discussing the engineering career to our class of freshman students. Among many brilliant remarks, one that stuck with me until now - more than 20 years later - was about the nature of the work we do\u0026hellip; He said something along the lines of, \u0026ldquo;Engineers are the professionals closest to gods (\u0026hellip;) because we don\u0026rsquo;t just fix things, we create them.\u0026rdquo;\nYes, I know this comment out of context might sound weird, but the whole point of the conversation was related to the \u0026ldquo;God complex\u0026rdquo; that some careers like Medicine often struggle with, and my professor was making fun out of it by saying that engineers should not feel inferior to physicians because we are actually the ones who should have this complex. (Please keep the comments tidy, this was early 2000\u0026rsquo;s so we were too close to the 90\u0026rsquo;s :-)\nIt is a common saying from where I came from that at the bottom of every joke there is a bit of truth, so setting things aside I think that Dr. Mehl was into something\u0026hellip; Stripping away all the tooling, at our core, we are creators. The way we create evolves. It used to be with our bare hands and raw materials. Then came tools, computers, and robots. Now, we\u0026rsquo;re entering the age of AI.\nSo whenever you think about the engineering career - and whether is it going to die or not - think about the human need to create new things. We will always be iterating, improving and innovating. This is such a fundamental characteristic of human nature that I cannot believe it is going to go away, be it in the next 5 years or in the next millenium.\nBut the way we create and improve things, that yes, I believe it is going to change.\nOrchestrating, Not Just Coding # I think that the AI revolution will free us from the boring parts to focus on the most exciting aspect of our work: architecture, design, planning and testing the solutions.\nI\u0026rsquo;m not saying that coding itself is boring. Actually, quite the opposite, like most engineers I enjoy coding quite a lot. But maintaing professional code is different than writing code for fun. There are so many things that you will write that doesn\u0026rsquo;t entirely translate to the things you want to achieve.\nThink about all the non-functionals, like for example security, logging, metrics and networking; the hours spent setting up, deploying, testing and validating; all the setup steps to integrate different application layers, databases and so on\u0026hellip; A lot of the things that we do on the day to day basis is just to \u0026ldquo;make it work\u0026rdquo;, as opposite to actually solving a business problem.\nEven simple tasks, like setting up this blog, still took me a few days of work to do - I had to setup my development environment, research what is the best tool foor blogging in 2025, research hosting options, research the domain language used to describe blogs in my chosen platform (e.g. how Hugo sites are organised), test a few templates before selecting one, research how to add comments to the posts\u0026hellip; and so on.\nHow does all of this relates to the business problem I want to solve? They are important steps, but I\u0026rsquo;ve only done them because they are steps required to \u0026ldquo;make it work\u0026rdquo;, and not really what I wanted to achieve in the first place.\nWhat I actually want to create a source of truth that I can use to publish blog posts and then later broadcast them to different platforms for better reach (e.g. Medium, LinkedIn, etc.). I still haven\u0026rsquo;t been able to solve this business problem because I had to focus on all aspects of the development pipeline of a new app, from inception to production.\nHow great it would be if we could abstract all of that and just ask an AI to \u0026ldquo;generate a blog platform for me, batteries included\u0026rdquo;?\nThis is where AI is going to quickly become very powerful. With the birth of Agentic AI, we will soon have thousands of agents at our disposal to orchestrate all steps of the software development process, leaving us with the main task of ideation of new experiences, and the \u0026ldquo;operationalisation\u0026rdquo; of that will be responsibility of the agents. They will code, deploy and iterate. They will do the research for us and give us options, so we can pick and choose according to our ideals.\nIt might be hard for you to imagine how this would work without a concrete example, but that\u0026rsquo;s why I love science fiction as it allows us to have a peek at the future. Think about Star Trek\u0026rsquo;s computer\u0026hellip; I believe that in a few years we will be interacting with the computer in the same way Geordy La Forge does in this scene - the engineering process becomes a dialogue more than an one sided effort on an engineer.\nAre we there yet? Not really, but Agentic AI is the next step towards making the future depicted in Star Trek reality. It is definitely starting to feel like AI is bigger than the internet, and as developers we should be making the effort to update ourselves to the \u0026ldquo;next generation\u0026rdquo; (pun intented).\nSo, Can Anyone \u0026ldquo;Vibe Code\u0026rdquo;? # Maybe not anyone, but the barrier to entry is definitely getting lower. The core skills are evolving, but the engineer\u0026rsquo;s role of creating and architecting is still here and will be here for a long time.\nWhat do you think? Can you see yourself as a more \u0026ldquo;hands-off\u0026rdquo; engineer, guiding AI to bring your visions to life? Let\u0026rsquo;s discuss in the comments!\n","date":"28 May 2025","externalUrl":null,"permalink":"/posts/20250528-vibe-coding/","section":"Posts","summary":"A reflection on the future of the software engineering career.","title":"Can anyone vibe code?","type":"posts"},{"content":"","date":"28 May 2025","externalUrl":null,"permalink":"/pt-br/tags/opiniao/","section":"Tags","summary":"","title":"Opiniao","type":"tags"},{"content":"","date":"28 May 2025","externalUrl":null,"permalink":"/tags/opinion/","section":"Tags","summary":"","title":"Opinion","type":"tags"},{"content":"Alright everyone, let\u0026rsquo;s talk about Jules! Hot from the ovens of Google I/O, this is what Google is calling an autonomous coding agent… but what is an autonomous coding agent? Think about NotebookLM, but for coding - a specialised AI to help you with coding tasks. The main difference from the traditional “vibe coding” approach is that with Jules you can import your whole project as context for the AI, so all the responses are grounded on the code you are actually working on!\nOnce the project is imported, you can interact with Jules by submitting “tasks”, which can be anything from bug fixes, dependency updates, new features, planning, documentation, tests and so on. As soon as it receives a task Jules will asynchronously plan its execution in steps and perform different sub-tasks to ensure that the desired outcome is achieved. For example, ensuring that no tests were broken by the new change.\nIt integrates directly with GitHub, so there is very little friction to start using it. It still won’t replace the IDE completely, but you can perform many tasks directly from Jules up to the point it creates a branch with all the alterations you requested ready to be made into a pull request.\nThe unfortunate consequence of Jules announcement yesterday is that the tool is currently under heavy load, so it might take a while after you submit a task to see the results, but Jules will do the work in background and if you have browser notifications enabled it will let you know once it is ready.\nGiven this fact, I wasn’t able to do any major experiments with it, but one of the things I did was to generate the README for my blog project on Github (the source for this very page you are reading now). I also tried some more complex iterations like adjusting the blog template. It did generate the correct files, but it was a bit slow to respond to my requests so I had to do a few changes manually.\nNot bad for day 1 I would say, and there is a lot of potential to be unlocked over the next few weeks and months. The killer feature is the ability to work on a complete code base instead of that traditional flow of asking Gemini (or ChatGPT) a question, copying the source to the IDE, running, copying and pasting back the results to the LLM and iterating. Of course, tools like Code Assist and CoPilot will provide some of those capabilities without leaving the IDE, but I still feel that the IDE is not the right environment for vibe coding as it feels more like a hack.\nIn that spirit, maybe Jules is the injection of inspiration we needed for a new era of IDEs that will unlock the potential of AI for developers all over the world in a more natural way. At least that is what I am hoping for!\nJules is currently in public beta and you can play with it today by signing up at https://jules.google.\n","date":"21 May 2025","externalUrl":null,"permalink":"/posts/20250521-jules/","section":"Posts","summary":"The new autonomous coding agent that every developer needs to know.","title":"We need to talk about Jules!","type":"posts"},{"content":"Welcome to danicat.dev! I\u0026rsquo;m Daniela Petruzalek, a software engineer with over 20 years of experience in the industry and currently working as a Developer Relations Engineer at Google. This is my new personal blog where I am going to share the latest developments in technology!\nIf you are already familiar with my content from my days as Google Developer Expert you might know what to expect, but if not please check out my previous content at my public speaking GitHub repository!\nI am always open for collaborations and speaking engagements. If you want to see me at your next event, or collab with me in a video or podcast, please connect with me on LinkedIn or send me a message at daniela@danicat.dev\nThat is it for now, but stay tuned for content about software engineering, Google Cloud, GenAI, data engineering and much more!\nDani =^.^=\nP.S.: Please note that the views written on this blog are my own and do not represent the views of my employer! ^^\n","date":"20 May 2025","externalUrl":null,"permalink":"/posts/20250520-hello-world/","section":"Posts","summary":"Just a few words to kickstart this blog!","title":"Hello World","type":"posts"},{"content":"","externalUrl":null,"permalink":"/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":"","externalUrl":null,"permalink":"/series/","section":"Series","summary":"","title":"Series","type":"series"}]